{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c393c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'\n",
    "# os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56dc7ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.qwen2_vl.video_processing_qwen2_vl import Qwen2VLVideoProcessor\n",
    "from transformers.video_utils import VideoMetadata\n",
    "import torch\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ceebdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedFrameQwen2VLVideoProcessor(Qwen2VLVideoProcessor):\n",
    "    def __init__(self, num_frames=50, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fixed_num_frames = num_frames\n",
    "\n",
    "    def sample_frames(\n",
    "        self,\n",
    "        metadata: VideoMetadata,\n",
    "        temporal_patch_size: Optional[int] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        temporal_patch_size = temporal_patch_size or self.temporal_patch_size\n",
    "\n",
    "        # 对齐 temporal_patch_size（Qwen2.5 默认 = 2）\n",
    "        num_frames = round(self.fixed_num_frames / temporal_patch_size) * temporal_patch_size\n",
    "        num_frames = min(num_frames, metadata.total_num_frames)\n",
    "\n",
    "        indices = torch.linspace(\n",
    "            0,\n",
    "            metadata.total_num_frames - 1,\n",
    "            steps=num_frames,\n",
    "        ).long()\n",
    "\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97963d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 09:34:30,227 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: ../../Qwen/cache/modelscope/Qwen/Qwen2.5-Omni-3B\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "from modelscope import snapshot_download\n",
    "from modelscope import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\n",
    "from qwen_omni_utils import process_mm_info\n",
    "\n",
    "model_dir = snapshot_download(\n",
    "    'Qwen/Qwen2.5-Omni-3B',\n",
    "    cache_dir=\"../../Qwen/cache/modelscope\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f7b330d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "You are attempting to use Flash Attention 2 without specifying a torch dtype. This might lead to unexpected behaviour\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f565b0d5474c11ad63dd1b6510d526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    }
   ],
   "source": [
    "model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"balanced\",\n",
    "    dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model.disable_talker()\n",
    "\n",
    "# video_processor = FixedFrameQwen2VLVideoProcessor.from_pretrained(model_dir)\n",
    "# processor = Qwen2_5OmniProcessor.from_pretrained(\n",
    "#     model_dir,\n",
    "#     video_processor=video_processor,\n",
    "# )\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(\n",
    "    model_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b6b31a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/qwen_omni_utils/v2_5/audio_process.py:57: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audios.append(librosa.load(path, sr=16000)[0])\n",
      "/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "qwen-vl-utils using torchvision to read video.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (53561 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_grid_thw: tensor([[165,  26,  46]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"video\", \"video\": \"./test.mp4\"},\n",
    "            {\"type\": \"text\", \"text\": \"Could you detail events during different time segments? Format strictly:\\nFrom xx to xx, event1.\\nFrom xx to xx, event2.\\n...\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "USE_AUDIO_IN_VIDEO = True\n",
    "\n",
    "text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "inputs = inputs.to(model.device).to(model.dtype)\n",
    "\n",
    "print(\"video_grid_thw:\", inputs[\"video_grid_thw\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70d2f7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"system\\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\\nuser\\nCould you detail events during different time segments? Format strictly:\\nFrom xx to xx, event1.\\nFrom xx to xx, event2.\\n...\\nassistant\\nSure! Here's a breakdown of the events during different time segments:\\n\\nFrom 0.00 to 10.00, the video starts with a close-up of a white object against a dark background.\\nFrom 10.00 to 10.64, the video transitions to a black screen with a white logo in the center.\\nFrom 10.64 to 11.28, the video shows a black screen with a white logo in the center.\\nFrom 11.28 to 12.00, the video transitions to a black screen with a white logo in the center.\\nFrom 12.00 to 12.64, the video shows a black screen with a white logo in the center.\\nFrom 12.64 to 13.28, the video transitions to a black screen with a white logo in the center.\\nFrom 13.28 to 14.00, the video shows a black screen with a white logo in the center.\\nFrom 14.00 to 14.64, the video transitions to a black screen with a white logo in the center.\\nFrom 14.64 to 15.28, the video shows a black screen with a white logo in the center.\\nFrom 15.28 to 16.00, the video transitions to a black screen with a white logo in the center.\\nFrom 16.00 to 16.64, the video shows a black screen with a white logo in the center.\\nFrom 16.64 to 17.28, the video transitions to a black screen with a white logo in the center.\\nFrom 17.28 to 18.00, the video shows a black screen with a white logo in the center.\\nFrom 18.00 to 18.64, the video transitions to a black screen with a white logo in the center.\\nFrom 18.64 to 19.28, the video shows a black screen with a white logo in the center.\\nFrom 19.28 to 20.00, the video transitions to a black screen with a white logo in the center.\\nFrom 20.00 to 20.64, the video shows a black screen with a white logo in the center.\\nFrom 20.64 to 21.28, the video transitions to a black screen with a white logo in the center.\\nFrom 21.28 to 22.00, the video shows a black screen with a white logo in the center.\\nFrom 22.00 to 22.64, the video transitions to a black screen with a white logo in the center.\\nFrom 22.64 to 23.28, the video shows a black screen with a white logo in the center.\\nFrom 23.28 to 24.00, the video transitions to a black screen with a white logo in the center.\\nFrom 24.00 to 24.64, the video shows a black screen with a white logo in the center.\\nFrom 24.64 to 25.28, the video transitions to a black screen with a white logo in the center.\\nFrom 25.28 to 26.00, the video shows a black screen with a white logo in the center.\\nFrom 26.00 to 26.64, the video transitions to a black screen with a white logo in the center.\\nFrom 26.64 to 27.28, the video shows a black screen with a white logo in the center.\\nFrom 27.28 to 28.00, the video transitions to a black screen with a white logo in the center.\\nFrom 28.00 to 28.64, the video shows a black screen with a white logo in the center.\\nFrom 28.64 to 29.28, the video transitions to a black screen with a white logo in the center.\\nFrom 29.28 to 30.00, the video shows a black screen with a white logo in the center.\\nFrom 30.00 to 30.64, the video transitions to a black screen with a white logo in the center.\\nFrom 30.64 to 31.28, the video shows a black screen with a white logo in the center.\\nFrom 31.28 to 32.00, the video transitions to a black screen with a white logo in the center.\\nFrom 32.0\"]\n"
     ]
    }
   ],
   "source": [
    "text_ids = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO, return_audio=False)\n",
    "\n",
    "text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7d71f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([197340, 1176])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"pixel_values_videos\"].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
