{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef80fea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'\n",
    "# os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76c9691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from modelscope import snapshot_download\n",
    "from modelscope import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor, Qwen2_5OmniThinkerForConditionalGeneration\n",
    "\n",
    "from transformers.models.qwen2_vl.video_processing_qwen2_vl import Qwen2VLVideoProcessor\n",
    "from transformers.video_utils import VideoMetadata\n",
    "from typing import Optional\n",
    "from qwen_omni_utils import process_mm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb616b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class OmniVideoConversationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        json_path: str,\n",
    "        video_root: str,\n",
    "    ):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "        self.video_root = video_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _build_text(self, conversations):\n",
    "        messages = []\n",
    "        for turn in conversations:\n",
    "            if turn[\"from\"] == \"human\":\n",
    "                role = \"user\"\n",
    "            elif turn[\"from\"] == \"gpt\":\n",
    "                role = \"assistant\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            messages.append({\n",
    "                \"role\": role,\n",
    "                \"content\": turn[\"value\"]\n",
    "            })\n",
    "\n",
    "        return messages\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        messages = self._build_text(sample[\"conversations\"])\n",
    "        video_id = sample[\"id\"]\n",
    "        video_path = os.path.join(self.video_root, f\"{video_id}.mp4\")\n",
    "\n",
    "        return {\n",
    "            \"text\": messages,\n",
    "            \"videos\": [video_path],\n",
    "        }\n",
    "\n",
    "def build_prompt(messages):\n",
    "    prompt = \"\"\n",
    "    for m in messages:\n",
    "        if m[\"role\"] == \"user\":\n",
    "            prompt += f\"<human>{m['content']}</human>\"\n",
    "        elif m[\"role\"] == \"assistant\":\n",
    "            prompt += f\"<gpt>{m['content']}</gpt>\"\n",
    "    return prompt\n",
    "\n",
    "class QwenOmniDataCollator:\n",
    "    def __init__(self, processor, num_frames=50):\n",
    "        self.processor = processor\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def __call__(self, features):\n",
    "        texts = [build_prompt(f[\"text\"]) for f in features]\n",
    "        print(texts[0])\n",
    "\n",
    "        # 视频路径列表\n",
    "        videos = [f[\"videos\"][0] if f.get(\"videos\") else None for f in features]\n",
    "\n",
    "        \n",
    "        # text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "        # audios, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "        batch = self.processor(\n",
    "            text=texts,\n",
    "            # audio=audios,\n",
    "            videos=videos,\n",
    "            video_num_frames=self.num_frames,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            use_audio_in_video=True\n",
    "        )\n",
    "\n",
    "        print(batch[\"video_grid_thw\"].shape)\n",
    "        print(batch[\"pixel_values_videos\"].shape)\n",
    "        \n",
    "        for k, v in batch.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                print(k, v.shape, v.numel() * v.element_size() / 1024**3, \"GB\")\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "class FixedFrameQwen2VLVideoProcessor(Qwen2VLVideoProcessor):\n",
    "    def __init__(self, num_frames=50, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fixed_num_frames = num_frames\n",
    "\n",
    "    def sample_frames(\n",
    "        self,\n",
    "        metadata: VideoMetadata,\n",
    "        temporal_patch_size: Optional[int] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        temporal_patch_size = temporal_patch_size or self.temporal_patch_size\n",
    "\n",
    "        # 对齐 temporal_patch_size（Qwen2.5 默认 = 2）\n",
    "        num_frames = round(self.fixed_num_frames / temporal_patch_size) * temporal_patch_size\n",
    "        num_frames = min(num_frames, metadata.total_num_frames)\n",
    "\n",
    "        indices = torch.linspace(\n",
    "            0,\n",
    "            metadata.total_num_frames - 1,\n",
    "            steps=num_frames,\n",
    "        ).long()\n",
    "\n",
    "        return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09feda9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = OmniVideoConversationDataset(\n",
    "    json_path=\"../../LongVALE/data/longvale-sft-bp-7k.json\",\n",
    "    video_root=\"../../LongVALE/raw_videos_train/video_train_7240/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5663cb",
   "metadata": {},
   "source": [
    "thinker\n",
    "\n",
    "github上看到还有就是用 model= model.thinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9235377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: ../../Qwen/cache/modelscope/Qwen/Qwen2.5-Omni-3B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 07:22:06,253 - modelscope - INFO - Target directory already exists, skipping creation.\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a085405a504d7696bb61af38842b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    }
   ],
   "source": [
    "model_path = snapshot_download(\n",
    "    'Qwen/Qwen2.5-Omni-3B',\n",
    "    cache_dir=\"../../Qwen/cache/modelscope\"\n",
    ")\n",
    "\n",
    "model = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"balanced\",\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "video_processor = FixedFrameQwen2VLVideoProcessor.from_pretrained(model_path)\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(\n",
    "    model_path,\n",
    "    video_processor=video_processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa1b05",
   "metadata": {},
   "source": [
    "```shell\n",
    "           ┌─ gate_proj ── activation ──┐\n",
    "x ──┬──────┤                            ⊙ ── down_proj ── out\n",
    "    │      └─ up_proj   ───────────────┘\n",
    "```\n",
    "\n",
    "+ 不调 gate_proj\n",
    "\n",
    "    trainable params: 31,158,272 || all params: 4,734,622,720 || trainable%: 0.6581\n",
    "\n",
    "+ 调 gate_proj\n",
    "\n",
    "    trainable params: 41,084,928 || all params: 4,744,549,376 || trainable%: 0.8659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d74b3f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 22,413,312 || all params: 4,734,622,720 || trainable%: 0.4734\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"up_proj\", \"down_proj\"],\n",
    "    # target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if (\n",
    "        \"audio_tower\" in name\n",
    "        or \"visual\" in name\n",
    "    ):\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./r_models\",\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=2,\n",
    "    # per_device_eval_batch_size=batch_size,\n",
    "    bf16=True,\n",
    "    num_train_epochs=2, # 5 -> 2\n",
    "    logging_steps=5,\n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=False,\n",
    "    label_names=[\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d611e96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.audio_tower.layers.0.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.0.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.0.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.1.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.1.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.1.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.2.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.2.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.2.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.3.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.3.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.3.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.4.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.4.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.4.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.5.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.5.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.5.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.6.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.6.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.6.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.7.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.7.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.7.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.8.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.8.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.8.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.9.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.9.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.9.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.10.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.10.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.10.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.11.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.11.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.11.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.12.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.12.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.12.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.13.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.13.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.13.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.14.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.14.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.14.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.15.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.15.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.15.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.16.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.16.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.16.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.17.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.17.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.17.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.18.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.18.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.18.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.19.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.19.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.19.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.20.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.20.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.20.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.21.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.21.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.21.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.22.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.22.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.22.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.23.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.23.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.23.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.24.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.24.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.24.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.25.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.25.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.25.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.26.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.26.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.26.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.27.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.27.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.27.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.28.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.28.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.28.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.29.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.29.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.29.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.30.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.30.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.30.self_attn.q_proj\n",
      "base_model.model.audio_tower.layers.31.self_attn.k_proj\n",
      "base_model.model.audio_tower.layers.31.self_attn.v_proj\n",
      "base_model.model.audio_tower.layers.31.self_attn.q_proj\n",
      "base_model.model.visual.blocks.0.mlp.up_proj\n",
      "base_model.model.visual.blocks.0.mlp.down_proj\n",
      "base_model.model.visual.blocks.1.mlp.up_proj\n",
      "base_model.model.visual.blocks.1.mlp.down_proj\n",
      "base_model.model.visual.blocks.2.mlp.up_proj\n",
      "base_model.model.visual.blocks.2.mlp.down_proj\n",
      "base_model.model.visual.blocks.3.mlp.up_proj\n",
      "base_model.model.visual.blocks.3.mlp.down_proj\n",
      "base_model.model.visual.blocks.4.mlp.up_proj\n",
      "base_model.model.visual.blocks.4.mlp.down_proj\n",
      "base_model.model.visual.blocks.5.mlp.up_proj\n",
      "base_model.model.visual.blocks.5.mlp.down_proj\n",
      "base_model.model.visual.blocks.6.mlp.up_proj\n",
      "base_model.model.visual.blocks.6.mlp.down_proj\n",
      "base_model.model.visual.blocks.7.mlp.up_proj\n",
      "base_model.model.visual.blocks.7.mlp.down_proj\n",
      "base_model.model.visual.blocks.8.mlp.up_proj\n",
      "base_model.model.visual.blocks.8.mlp.down_proj\n",
      "base_model.model.visual.blocks.9.mlp.up_proj\n",
      "base_model.model.visual.blocks.9.mlp.down_proj\n",
      "base_model.model.visual.blocks.10.mlp.up_proj\n",
      "base_model.model.visual.blocks.10.mlp.down_proj\n",
      "base_model.model.visual.blocks.11.mlp.up_proj\n",
      "base_model.model.visual.blocks.11.mlp.down_proj\n",
      "base_model.model.visual.blocks.12.mlp.up_proj\n",
      "base_model.model.visual.blocks.12.mlp.down_proj\n",
      "base_model.model.visual.blocks.13.mlp.up_proj\n",
      "base_model.model.visual.blocks.13.mlp.down_proj\n",
      "base_model.model.visual.blocks.14.mlp.up_proj\n",
      "base_model.model.visual.blocks.14.mlp.down_proj\n",
      "base_model.model.visual.blocks.15.mlp.up_proj\n",
      "base_model.model.visual.blocks.15.mlp.down_proj\n",
      "base_model.model.visual.blocks.16.mlp.up_proj\n",
      "base_model.model.visual.blocks.16.mlp.down_proj\n",
      "base_model.model.visual.blocks.17.mlp.up_proj\n",
      "base_model.model.visual.blocks.17.mlp.down_proj\n",
      "base_model.model.visual.blocks.18.mlp.up_proj\n",
      "base_model.model.visual.blocks.18.mlp.down_proj\n",
      "base_model.model.visual.blocks.19.mlp.up_proj\n",
      "base_model.model.visual.blocks.19.mlp.down_proj\n",
      "base_model.model.visual.blocks.20.mlp.up_proj\n",
      "base_model.model.visual.blocks.20.mlp.down_proj\n",
      "base_model.model.visual.blocks.21.mlp.up_proj\n",
      "base_model.model.visual.blocks.21.mlp.down_proj\n",
      "base_model.model.visual.blocks.22.mlp.up_proj\n",
      "base_model.model.visual.blocks.22.mlp.down_proj\n",
      "base_model.model.visual.blocks.23.mlp.up_proj\n",
      "base_model.model.visual.blocks.23.mlp.down_proj\n",
      "base_model.model.visual.blocks.24.mlp.up_proj\n",
      "base_model.model.visual.blocks.24.mlp.down_proj\n",
      "base_model.model.visual.blocks.25.mlp.up_proj\n",
      "base_model.model.visual.blocks.25.mlp.down_proj\n",
      "base_model.model.visual.blocks.26.mlp.up_proj\n",
      "base_model.model.visual.blocks.26.mlp.down_proj\n",
      "base_model.model.visual.blocks.27.mlp.up_proj\n",
      "base_model.model.visual.blocks.27.mlp.down_proj\n",
      "base_model.model.visual.blocks.28.mlp.up_proj\n",
      "base_model.model.visual.blocks.28.mlp.down_proj\n",
      "base_model.model.visual.blocks.29.mlp.up_proj\n",
      "base_model.model.visual.blocks.29.mlp.down_proj\n",
      "base_model.model.visual.blocks.30.mlp.up_proj\n",
      "base_model.model.visual.blocks.30.mlp.down_proj\n",
      "base_model.model.visual.blocks.31.mlp.up_proj\n",
      "base_model.model.visual.blocks.31.mlp.down_proj\n",
      "base_model.model.model.layers.0.self_attn.q_proj\n",
      "base_model.model.model.layers.0.self_attn.k_proj\n",
      "base_model.model.model.layers.0.self_attn.v_proj\n",
      "base_model.model.model.layers.0.self_attn.o_proj\n",
      "base_model.model.model.layers.0.mlp.up_proj\n",
      "base_model.model.model.layers.0.mlp.down_proj\n",
      "base_model.model.model.layers.1.self_attn.q_proj\n",
      "base_model.model.model.layers.1.self_attn.k_proj\n",
      "base_model.model.model.layers.1.self_attn.v_proj\n",
      "base_model.model.model.layers.1.self_attn.o_proj\n",
      "base_model.model.model.layers.1.mlp.up_proj\n",
      "base_model.model.model.layers.1.mlp.down_proj\n",
      "base_model.model.model.layers.2.self_attn.q_proj\n",
      "base_model.model.model.layers.2.self_attn.k_proj\n",
      "base_model.model.model.layers.2.self_attn.v_proj\n",
      "base_model.model.model.layers.2.self_attn.o_proj\n",
      "base_model.model.model.layers.2.mlp.up_proj\n",
      "base_model.model.model.layers.2.mlp.down_proj\n",
      "base_model.model.model.layers.3.self_attn.q_proj\n",
      "base_model.model.model.layers.3.self_attn.k_proj\n",
      "base_model.model.model.layers.3.self_attn.v_proj\n",
      "base_model.model.model.layers.3.self_attn.o_proj\n",
      "base_model.model.model.layers.3.mlp.up_proj\n",
      "base_model.model.model.layers.3.mlp.down_proj\n",
      "base_model.model.model.layers.4.self_attn.q_proj\n",
      "base_model.model.model.layers.4.self_attn.k_proj\n",
      "base_model.model.model.layers.4.self_attn.v_proj\n",
      "base_model.model.model.layers.4.self_attn.o_proj\n",
      "base_model.model.model.layers.4.mlp.up_proj\n",
      "base_model.model.model.layers.4.mlp.down_proj\n",
      "base_model.model.model.layers.5.self_attn.q_proj\n",
      "base_model.model.model.layers.5.self_attn.k_proj\n",
      "base_model.model.model.layers.5.self_attn.v_proj\n",
      "base_model.model.model.layers.5.self_attn.o_proj\n",
      "base_model.model.model.layers.5.mlp.up_proj\n",
      "base_model.model.model.layers.5.mlp.down_proj\n",
      "base_model.model.model.layers.6.self_attn.q_proj\n",
      "base_model.model.model.layers.6.self_attn.k_proj\n",
      "base_model.model.model.layers.6.self_attn.v_proj\n",
      "base_model.model.model.layers.6.self_attn.o_proj\n",
      "base_model.model.model.layers.6.mlp.up_proj\n",
      "base_model.model.model.layers.6.mlp.down_proj\n",
      "base_model.model.model.layers.7.self_attn.q_proj\n",
      "base_model.model.model.layers.7.self_attn.k_proj\n",
      "base_model.model.model.layers.7.self_attn.v_proj\n",
      "base_model.model.model.layers.7.self_attn.o_proj\n",
      "base_model.model.model.layers.7.mlp.up_proj\n",
      "base_model.model.model.layers.7.mlp.down_proj\n",
      "base_model.model.model.layers.8.self_attn.q_proj\n",
      "base_model.model.model.layers.8.self_attn.k_proj\n",
      "base_model.model.model.layers.8.self_attn.v_proj\n",
      "base_model.model.model.layers.8.self_attn.o_proj\n",
      "base_model.model.model.layers.8.mlp.up_proj\n",
      "base_model.model.model.layers.8.mlp.down_proj\n",
      "base_model.model.model.layers.9.self_attn.q_proj\n",
      "base_model.model.model.layers.9.self_attn.k_proj\n",
      "base_model.model.model.layers.9.self_attn.v_proj\n",
      "base_model.model.model.layers.9.self_attn.o_proj\n",
      "base_model.model.model.layers.9.mlp.up_proj\n",
      "base_model.model.model.layers.9.mlp.down_proj\n",
      "base_model.model.model.layers.10.self_attn.q_proj\n",
      "base_model.model.model.layers.10.self_attn.k_proj\n",
      "base_model.model.model.layers.10.self_attn.v_proj\n",
      "base_model.model.model.layers.10.self_attn.o_proj\n",
      "base_model.model.model.layers.10.mlp.up_proj\n",
      "base_model.model.model.layers.10.mlp.down_proj\n",
      "base_model.model.model.layers.11.self_attn.q_proj\n",
      "base_model.model.model.layers.11.self_attn.k_proj\n",
      "base_model.model.model.layers.11.self_attn.v_proj\n",
      "base_model.model.model.layers.11.self_attn.o_proj\n",
      "base_model.model.model.layers.11.mlp.up_proj\n",
      "base_model.model.model.layers.11.mlp.down_proj\n",
      "base_model.model.model.layers.12.self_attn.q_proj\n",
      "base_model.model.model.layers.12.self_attn.k_proj\n",
      "base_model.model.model.layers.12.self_attn.v_proj\n",
      "base_model.model.model.layers.12.self_attn.o_proj\n",
      "base_model.model.model.layers.12.mlp.up_proj\n",
      "base_model.model.model.layers.12.mlp.down_proj\n",
      "base_model.model.model.layers.13.self_attn.q_proj\n",
      "base_model.model.model.layers.13.self_attn.k_proj\n",
      "base_model.model.model.layers.13.self_attn.v_proj\n",
      "base_model.model.model.layers.13.self_attn.o_proj\n",
      "base_model.model.model.layers.13.mlp.up_proj\n",
      "base_model.model.model.layers.13.mlp.down_proj\n",
      "base_model.model.model.layers.14.self_attn.q_proj\n",
      "base_model.model.model.layers.14.self_attn.k_proj\n",
      "base_model.model.model.layers.14.self_attn.v_proj\n",
      "base_model.model.model.layers.14.self_attn.o_proj\n",
      "base_model.model.model.layers.14.mlp.up_proj\n",
      "base_model.model.model.layers.14.mlp.down_proj\n",
      "base_model.model.model.layers.15.self_attn.q_proj\n",
      "base_model.model.model.layers.15.self_attn.k_proj\n",
      "base_model.model.model.layers.15.self_attn.v_proj\n",
      "base_model.model.model.layers.15.self_attn.o_proj\n",
      "base_model.model.model.layers.15.mlp.up_proj\n",
      "base_model.model.model.layers.15.mlp.down_proj\n",
      "base_model.model.model.layers.16.self_attn.q_proj\n",
      "base_model.model.model.layers.16.self_attn.k_proj\n",
      "base_model.model.model.layers.16.self_attn.v_proj\n",
      "base_model.model.model.layers.16.self_attn.o_proj\n",
      "base_model.model.model.layers.16.mlp.up_proj\n",
      "base_model.model.model.layers.16.mlp.down_proj\n",
      "base_model.model.model.layers.17.self_attn.q_proj\n",
      "base_model.model.model.layers.17.self_attn.k_proj\n",
      "base_model.model.model.layers.17.self_attn.v_proj\n",
      "base_model.model.model.layers.17.self_attn.o_proj\n",
      "base_model.model.model.layers.17.mlp.up_proj\n",
      "base_model.model.model.layers.17.mlp.down_proj\n",
      "base_model.model.model.layers.18.self_attn.q_proj\n",
      "base_model.model.model.layers.18.self_attn.k_proj\n",
      "base_model.model.model.layers.18.self_attn.v_proj\n",
      "base_model.model.model.layers.18.self_attn.o_proj\n",
      "base_model.model.model.layers.18.mlp.up_proj\n",
      "base_model.model.model.layers.18.mlp.down_proj\n",
      "base_model.model.model.layers.19.self_attn.q_proj\n",
      "base_model.model.model.layers.19.self_attn.k_proj\n",
      "base_model.model.model.layers.19.self_attn.v_proj\n",
      "base_model.model.model.layers.19.self_attn.o_proj\n",
      "base_model.model.model.layers.19.mlp.up_proj\n",
      "base_model.model.model.layers.19.mlp.down_proj\n",
      "base_model.model.model.layers.20.self_attn.q_proj\n",
      "base_model.model.model.layers.20.self_attn.k_proj\n",
      "base_model.model.model.layers.20.self_attn.v_proj\n",
      "base_model.model.model.layers.20.self_attn.o_proj\n",
      "base_model.model.model.layers.20.mlp.up_proj\n",
      "base_model.model.model.layers.20.mlp.down_proj\n",
      "base_model.model.model.layers.21.self_attn.q_proj\n",
      "base_model.model.model.layers.21.self_attn.k_proj\n",
      "base_model.model.model.layers.21.self_attn.v_proj\n",
      "base_model.model.model.layers.21.self_attn.o_proj\n",
      "base_model.model.model.layers.21.mlp.up_proj\n",
      "base_model.model.model.layers.21.mlp.down_proj\n",
      "base_model.model.model.layers.22.self_attn.q_proj\n",
      "base_model.model.model.layers.22.self_attn.k_proj\n",
      "base_model.model.model.layers.22.self_attn.v_proj\n",
      "base_model.model.model.layers.22.self_attn.o_proj\n",
      "base_model.model.model.layers.22.mlp.up_proj\n",
      "base_model.model.model.layers.22.mlp.down_proj\n",
      "base_model.model.model.layers.23.self_attn.q_proj\n",
      "base_model.model.model.layers.23.self_attn.k_proj\n",
      "base_model.model.model.layers.23.self_attn.v_proj\n",
      "base_model.model.model.layers.23.self_attn.o_proj\n",
      "base_model.model.model.layers.23.mlp.up_proj\n",
      "base_model.model.model.layers.23.mlp.down_proj\n",
      "base_model.model.model.layers.24.self_attn.q_proj\n",
      "base_model.model.model.layers.24.self_attn.k_proj\n",
      "base_model.model.model.layers.24.self_attn.v_proj\n",
      "base_model.model.model.layers.24.self_attn.o_proj\n",
      "base_model.model.model.layers.24.mlp.up_proj\n",
      "base_model.model.model.layers.24.mlp.down_proj\n",
      "base_model.model.model.layers.25.self_attn.q_proj\n",
      "base_model.model.model.layers.25.self_attn.k_proj\n",
      "base_model.model.model.layers.25.self_attn.v_proj\n",
      "base_model.model.model.layers.25.self_attn.o_proj\n",
      "base_model.model.model.layers.25.mlp.up_proj\n",
      "base_model.model.model.layers.25.mlp.down_proj\n",
      "base_model.model.model.layers.26.self_attn.q_proj\n",
      "base_model.model.model.layers.26.self_attn.k_proj\n",
      "base_model.model.model.layers.26.self_attn.v_proj\n",
      "base_model.model.model.layers.26.self_attn.o_proj\n",
      "base_model.model.model.layers.26.mlp.up_proj\n",
      "base_model.model.model.layers.26.mlp.down_proj\n",
      "base_model.model.model.layers.27.self_attn.q_proj\n",
      "base_model.model.model.layers.27.self_attn.k_proj\n",
      "base_model.model.model.layers.27.self_attn.v_proj\n",
      "base_model.model.model.layers.27.self_attn.o_proj\n",
      "base_model.model.model.layers.27.mlp.up_proj\n",
      "base_model.model.model.layers.27.mlp.down_proj\n",
      "base_model.model.model.layers.28.self_attn.q_proj\n",
      "base_model.model.model.layers.28.self_attn.k_proj\n",
      "base_model.model.model.layers.28.self_attn.v_proj\n",
      "base_model.model.model.layers.28.self_attn.o_proj\n",
      "base_model.model.model.layers.28.mlp.up_proj\n",
      "base_model.model.model.layers.28.mlp.down_proj\n",
      "base_model.model.model.layers.29.self_attn.q_proj\n",
      "base_model.model.model.layers.29.self_attn.k_proj\n",
      "base_model.model.model.layers.29.self_attn.v_proj\n",
      "base_model.model.model.layers.29.self_attn.o_proj\n",
      "base_model.model.model.layers.29.mlp.up_proj\n",
      "base_model.model.model.layers.29.mlp.down_proj\n",
      "base_model.model.model.layers.30.self_attn.q_proj\n",
      "base_model.model.model.layers.30.self_attn.k_proj\n",
      "base_model.model.model.layers.30.self_attn.v_proj\n",
      "base_model.model.model.layers.30.self_attn.o_proj\n",
      "base_model.model.model.layers.30.mlp.up_proj\n",
      "base_model.model.model.layers.30.mlp.down_proj\n",
      "base_model.model.model.layers.31.self_attn.q_proj\n",
      "base_model.model.model.layers.31.self_attn.k_proj\n",
      "base_model.model.model.layers.31.self_attn.v_proj\n",
      "base_model.model.model.layers.31.self_attn.o_proj\n",
      "base_model.model.model.layers.31.mlp.up_proj\n",
      "base_model.model.model.layers.31.mlp.down_proj\n",
      "base_model.model.model.layers.32.self_attn.q_proj\n",
      "base_model.model.model.layers.32.self_attn.k_proj\n",
      "base_model.model.model.layers.32.self_attn.v_proj\n",
      "base_model.model.model.layers.32.self_attn.o_proj\n",
      "base_model.model.model.layers.32.mlp.up_proj\n",
      "base_model.model.model.layers.32.mlp.down_proj\n",
      "base_model.model.model.layers.33.self_attn.q_proj\n",
      "base_model.model.model.layers.33.self_attn.k_proj\n",
      "base_model.model.model.layers.33.self_attn.v_proj\n",
      "base_model.model.model.layers.33.self_attn.o_proj\n",
      "base_model.model.model.layers.33.mlp.up_proj\n",
      "base_model.model.model.layers.33.mlp.down_proj\n",
      "base_model.model.model.layers.34.self_attn.q_proj\n",
      "base_model.model.model.layers.34.self_attn.k_proj\n",
      "base_model.model.model.layers.34.self_attn.v_proj\n",
      "base_model.model.model.layers.34.self_attn.o_proj\n",
      "base_model.model.model.layers.34.mlp.up_proj\n",
      "base_model.model.model.layers.34.mlp.down_proj\n",
      "base_model.model.model.layers.35.self_attn.q_proj\n",
      "base_model.model.model.layers.35.self_attn.k_proj\n",
      "base_model.model.model.layers.35.self_attn.v_proj\n",
      "base_model.model.model.layers.35.self_attn.o_proj\n",
      "base_model.model.model.layers.35.mlp.up_proj\n",
      "base_model.model.model.layers.35.mlp.down_proj\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if hasattr(module, \"lora_A\"):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68724835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "data_collator = QwenOmniDataCollator(processor)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    # model=model.thinker,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    processing_class=processor,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fad01a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "Keyword argument `video_num_frames` is not a valid argument for this processor and will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human><video>\n",
      "Could you tell me what happened from <s3> to <e3> in the video?</human><gpt>In a wooded area, people gather around a white tent, chatting and preparing for an outdoor adventure, while a man carrying a backpack walks purposefully through the woods.</gpt><human>Share what transpired from <s17> to <e17> in the video.</human><gpt>Under a large white tent, people in blue jackets gather around a dog lying on the ground. One person kneels beside it, while another lies down with their head resting on the dog's back. The dog remains calm and relaxed, enjoying the company of its human companions.</gpt><human>Between which two frames can we witness in a cozy living room, a woman with glasses and a blue sweatshirt sits on a beige sofa, smiling and sharing her thoughts and experiences with the viewer, while the soft lighting creates a warm and inviting atmosphere occurring in the video?</human><gpt>From <s21> to <e21>.</gpt><human>Explain what happened from <s8> to <e8> in the video.</human><gpt>In a serene park setting, a woman runs towards a group of people gathered under a white tent, while the camera follows her movements and captures the beauty of the surroundings.</gpt><human>Between which two frames can we witness in a lush park, a determined runner wearing a blue jacket and black pants approaches the camera, their strides and expressions captured as they navigate the course amidst colorful flags and cheering spectators occurring in the video?</human><gpt>From <s19> to <e19>.</gpt><human>At what point in the video can we witness in a tranquil forest, people stroll along a dirt path, their laughter and chatter blending with the rustling of leaves and the chirping of birds happening?</human><gpt>From <s7> to <e7>.</gpt><human>During which frames can we see in the darkness of the woods, a group of runners illuminate the path ahead with their headlamps, their steady footsteps echoing through the serene night happening in the video?</human><gpt>From <s0> to <e0>.</gpt><human>Between which frames is in a lively park, a woman in a black shirt jogs towards the camera, her face beaming with joy, while a group of people engage in various activities in the background, creating a vibrant and social atmosphere visible in the video?</human><gpt>From <s12> to <e12>.</gpt><human>Between which two frames in the video can we observe in a close-up shot, a pizza oven cooks pizzas, with the heat causing the cheese to melt and the crust to turn golden brown taking place?</human><gpt>From <s14> to <e14>.</gpt><human>Explain what happened from <s1> to <e1> in the video.</human><gpt>In a cozy living room, a woman in a blue hoodie sits on a beige sofa, her glasses reflecting the soft lighting. She smiles and engages in a conversation, her voice echoing in the warm and inviting atmosphere.</gpt><human>During which frames can we see in a serene wooded area, three individuals and a dog enjoy a leisurely walk, their voices mingling with the rustling of leaves and the sound of the dog's playful footsteps happening in the video?</human><gpt>From <s18> to <e18>.</gpt><human>Provide details about the events from <s20> to <e20> in the video.</human><gpt>In a wooded area, a runner wearing a bright purple shirt and black shorts runs down a dirt path, their rhythmic movements and the sound of their breathing creating a sense of tranquility and immersion in nature.</gpt><human>Could you tell me what happened from <s15> to <e15> in the video?</human><gpt>In a peaceful outdoor setting, a woman enjoys a slice of pizza while a brown dog rests comfortably on the grass, oblivious to the person standing nearby.</gpt><human>At what point in the video can we witness in a serene outdoor setting, a trail sign marks the entrance to a lush green forest, where a group of tents are set up in a clearing, and a person approaches the campsite with a backpack happening?</human><gpt>From <s4> to <e4>.</gpt><human>At which time interval in the video can we see in a verdant park, two people sit on a blanket, one gently petting a black dog while the other holds its leash, as the dog sits attentively on the ground occurring?</human><gpt>From <s16> to <e16>.</gpt><human>At what point in the video can we witness in a lush green forest, a woman runs with determination, her focused expression and steady pace conveying her commitment to her outdoor activity happening?</human><gpt>From <s9> to <e9>.</gpt><human>Tell me about the events from <s11> to <e11> in the video.</human><gpt>In a cozy living room, a woman in a blue hoodie sits on a sofa, speaking directly to the camera with an engaged expression, while a man's voice briefly interjects with an expletive.</gpt><human>Can you describe what occurred from <s10> to <e10> in the video?</human><gpt>In a serene outdoor setting, people stroll along a dirt path, their laughter and chatter blending with the rustling of leaves and the chirping of birds.</gpt><human>Tell me about the events from <s6> to <e6> in the video.</human><gpt>In a wooded area with fallen leaves on the ground, a woman wearing a white cap and glasses jogs along a dirt path, her focused expression and determined body language reflecting her commitment to her outdoor activity.</gpt><human>What was going on from <s5> to <e5> in the video?</human><gpt>In an outdoor setting, a large tarp is spread out on the ground, surrounded by various bags and items, indicating a camping or gathering scenario.</gpt><human>During which frames in the video can we observe in the video, a person carefully prepares a pizza, spreading sauce, cheese, meat, and vegetables evenly across the dough, showcasing the meticulous process of creating a delicious meal happening?</human><gpt>From <s13> to <e13>.</gpt><human>Between which two frames can we witness in a serene outdoor setting, people gather in a large white tent and around a white pickup truck in a lush green field, enjoying the beautiful natural surroundings occurring in the video?</human><gpt>From <s2> to <e2>.</gpt>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/video_processing_utils.py:879: UserWarning: `torchcodec` is not installed and cannot be used to decode the video by default. Falling back to `torchvision`. Note that `torchvision` decoding is deprecated and will be removed in future versions. \n",
      "  warnings.warn(\n",
      "/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/video_utils.py:524: UserWarning: Using `torchvision` for video decoding is deprecated and will be removed in future versions. Please use `torchcodec` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "torch.Size([9309664, 1176])\n",
      "input_ids torch.Size([1, 1469]) 1.0944902896881104e-05 GB\n",
      "attention_mask torch.Size([1, 1469]) 1.0944902896881104e-05 GB\n",
      "pixel_values_videos torch.Size([9309664, 1176]) 40.7850923538208 GB\n",
      "video_grid_thw torch.Size([1, 3]) 2.2351741790771484e-08 GB\n",
      "video_second_per_grid torch.Size([1]) 3.725290298461914e-09 GB\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 40.79 GiB. GPU 0 has a total capacity of 23.69 GiB of which 16.96 GiB is free. Process 69705 has 1.13 GiB memory in use. Process 77552 has 1.13 GiB memory in use. Including non-PyTorch memory, this process has 4.26 GiB memory in use. Of the allocated memory 3.93 GiB is allocated by PyTorch, and 43.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/trainer.py:2618\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2616\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2617\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[0;32m-> 2618\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2619\u001b[0m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[1;32m   2620\u001b[0m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[1;32m   2621\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_gradient_accumulation_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/trainer.py:5654\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches, device)\u001b[0m\n\u001b[1;32m   5652\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m   5653\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5654\u001b[0m         batch_samples\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   5655\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   5656\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/accelerate/data_loader.py:577\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m         current_batch \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[1;32m    579\u001b[0m     next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/accelerate/utils/operations.py:154\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    152\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:246\u001b[0m, in \u001b[0;36mBatchFeature.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m v\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: maybe_to(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:246\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m v\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: \u001b[43mmaybe_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:240\u001b[0m, in \u001b[0;36mBatchFeature.to.<locals>.maybe_to\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmaybe_to\u001b[39m(v):\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# check if v is a floating point\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_floating_point(v):\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;66;03m# cast and send to device\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;129;01mand\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m v\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 40.79 GiB. GPU 0 has a total capacity of 23.69 GiB of which 16.96 GiB is free. Process 69705 has 1.13 GiB memory in use. Process 77552 has 1.13 GiB memory in use. Including non-PyTorch memory, this process has 4.26 GiB memory in use. Of the allocated memory 3.93 GiB is allocated by PyTorch, and 43.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a8e12e",
   "metadata": {},
   "source": [
    "pixel_values_videos torch.Size([9309664, 1176]) 40.7850923538208 GB\n",
    "\n",
    "均匀采样 100 帧没生效？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c701e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.FixedFrameQwen2VLVideoProcessor'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(type(processor.video_processor))\n",
    "print(hasattr(processor.video_processor, \"sample_frames\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
