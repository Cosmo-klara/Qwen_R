{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c393c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'\n",
    "# os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56dc7ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.qwen2_vl.video_processing_qwen2_vl import Qwen2VLVideoProcessor\n",
    "from transformers.video_utils import VideoMetadata\n",
    "import torch\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ceebdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedFrameQwen2VLVideoProcessor(Qwen2VLVideoProcessor):\n",
    "    def __init__(self, num_frames=100, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fixed_num_frames = num_frames\n",
    "\n",
    "    def sample_frames(\n",
    "        self,\n",
    "        metadata: VideoMetadata,\n",
    "        temporal_patch_size: Optional[int] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        temporal_patch_size = temporal_patch_size or self.temporal_patch_size\n",
    "\n",
    "        # 对齐 temporal_patch_size（Qwen2.5 默认 = 2）\n",
    "        num_frames = round(self.fixed_num_frames / temporal_patch_size) * temporal_patch_size\n",
    "        num_frames = min(num_frames, metadata.total_num_frames)\n",
    "\n",
    "        indices = torch.linspace(\n",
    "            0,\n",
    "            metadata.total_num_frames - 1,\n",
    "            steps=num_frames,\n",
    "        ).long()\n",
    "\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97963d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-16 23:54:21,992 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: ../../Qwen/cache/modelscope/Qwen/Qwen2.5-Omni-3B\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "from modelscope import snapshot_download\n",
    "from modelscope import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\n",
    "from qwen_omni_utils import process_mm_info\n",
    "\n",
    "model_dir = snapshot_download(\n",
    "    'Qwen/Qwen2.5-Omni-3B',\n",
    "    cache_dir=\"../../Qwen/cache/modelscope\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f7b330d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306f9deebbd547159b05d15aa7b3497d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    }
   ],
   "source": [
    "model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"cuda:0\",\n",
    "    dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model.disable_talker()\n",
    "\n",
    "video_processor = FixedFrameQwen2VLVideoProcessor.from_pretrained(model_dir)\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(\n",
    "    model_dir,\n",
    "    video_processor=video_processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b6b31a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/qwen_omni_utils/v2_5/audio_process.py:57: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audios.append(librosa.load(path, sr=16000)[0])\n",
      "/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "qwen-vl-utils using torchvision to read video.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_grid_thw: tensor([[59, 26, 34]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"video\", \"video\": \"./test.mp4\"},\n",
    "            {\"type\": \"text\", \"text\": \"Could you detail events during different time segments? Format strictly:\\nFrom xx to xx, event1.\\nFrom xx to xx, event2.\\n...\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "USE_AUDIO_IN_VIDEO = True\n",
    "\n",
    "text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "inputs = inputs.to(model.device).to(model.dtype)\n",
    "\n",
    "print(\"video_grid_thw:\", inputs[\"video_grid_thw\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70d2f7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system\\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\\nuser\\nCould you detail events during different time segments? Format strictly:\\nFrom xx to xx, event1.\\nFrom xx to xx, event2.\\n...\\nassistant\\nFrom 0.00 to 10.00, a man is standing in front of a tree.\\nFrom 10.00 to 15.00, the man climbs up the tree.\\nFrom 15.00 to 20.00, the man climbs down the tree.\\nFrom 20.00 to 25.00, the man climbs up the tree again.\\nFrom 25.00 to 30.00, the man climbs down the tree.\\nFrom 30.00 to 35.00, the man climbs up the tree once more.\\nFrom 35.00 to 40.00, the man climbs down the tree.\\nFrom 40.00 to 45.00, the man climbs up the tree for the third time.\\nFrom 45.00 to 50.00, the man climbs down the tree.\\nFrom 50.00 to 55.00, the man climbs up the tree for the fourth time.\\nFrom 55.00 to 60.00, the man climbs down the tree.\\nFrom 60.00 to 65.00, the man climbs up the tree for the fifth time.\\nFrom 65.00 to 70.00, the man climbs down the tree.\\nFrom 70.00 to 75.00, the man climbs up the tree for the sixth time.\\nFrom 75.00 to 80.00, the man climbs down the tree.\\nFrom 80.00 to 85.00, the man climbs up the tree for the seventh time.\\nFrom 85.00 to 90.00, the man climbs down the tree.\\nFrom 90.00 to 95.00, the man climbs up the tree for the eighth time.\\nFrom 95.00 to 100.00, the man climbs down the tree.\\nFrom 100.00 to 105.00, the man climbs up the tree for the ninth time.\\nFrom 105.00 to 110.00, the man climbs down the tree.\\nFrom 110.00 to 115.00, the man climbs up the tree for the tenth time.\\nFrom 115.00 to 120.00, the man climbs down the tree.\\nFrom 120.00 to 125.00, the man climbs up the tree for the eleventh time.\\nFrom 125.00 to 130.00, the man climbs down the tree.\\nFrom 130.00 to 135.00, the man climbs up the tree for the twelfth time.\\nFrom 135.00 to 140.00, the man climbs down the tree.\\nFrom 140.00 to 145.00, the man climbs up the tree for the thirteenth time.\\nFrom 145.00 to 150.00, the man climbs down the tree.\\nFrom 150.00 to 155.00, the man climbs up the tree for the fourteenth time.\\nFrom 155.00 to 160.00, the man climbs down the tree.\\nFrom 160.00 to 165.00, the man climbs up the tree for the fifteenth time.\\nFrom 165.00 to 170.00, the man climbs down the tree.\\nFrom 170.00 to 175.00, the man climbs up the tree for the sixteenth time.\\nFrom 175.00 to 180.00, the man climbs down the tree.\\nFrom 180.00 to 185.00, the man climbs up the tree for the seventeenth time.\\nFrom 185.00 to 190.00, the man climbs down the tree.\\nFrom 190.00 to 195.00, the man climbs up the tree for the eighteenth time.\\nFrom 195.00 to 200.00, the man climbs down the tree.\\nFrom 200.00 to 205.00, the man climbs up the tree for the nineteenth time.\\nFrom 205.00 to 210.00, the man']\n"
     ]
    }
   ],
   "source": [
    "text_ids = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO, return_audio=False)\n",
    "\n",
    "text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
