{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c393c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "from transformers.models.qwen2_vl.video_processing_qwen2_vl import Qwen2VLVideoProcessor\n",
    "from transformers.video_utils import VideoMetadata\n",
    "import torch\n",
    "from typing import Optional\n",
    "import soundfile as sf\n",
    "from modelscope import snapshot_download\n",
    "from modelscope import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\n",
    "from qwen_omni_utils import process_mm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97963d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: ../../Qwen/cache/modelscope/Qwen/Qwen2.5-Omni-3B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 09:06:34,886 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "model_dir = snapshot_download(\n",
    "    'Qwen/Qwen2.5-Omni-3B',\n",
    "    cache_dir=\"../../Qwen/cache/modelscope\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b828186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.image_utils import SizeDict\n",
    "\n",
    "class FixedResQwen2VLVideoProcessor(Qwen2VLVideoProcessor):\n",
    "    def _preprocess(\n",
    "        self, videos, do_resize=True, size=None, interpolation=None, **kwargs\n",
    "    ):\n",
    "        # 固定分辨率\n",
    "        fixed_size = SizeDict(height=224, width=224)\n",
    "        for i, video in enumerate(videos):\n",
    "            videos[i] = self.resize(video, size=fixed_size, interpolation=interpolation)\n",
    "        return super()._preprocess(videos, do_resize=False, size=fixed_size, interpolation=interpolation, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f7b330d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "You are attempting to use Flash Attention 2 without specifying a torch dtype. This might lead to unexpected behaviour\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a202cff76247aab8d9fabc149ebda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    }
   ],
   "source": [
    "model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"balanced\",\n",
    "    dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model.disable_talker()\n",
    "\n",
    "video_processor = FixedResQwen2VLVideoProcessor.from_pretrained(model_dir)\n",
    "# video_processor.do_sample_frames = True\n",
    "# video_processor.fps = 2.0\n",
    "\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(\n",
    "    model_dir,\n",
    "    video_processor=video_processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b6b31a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/qwen_omni_utils/v2_5/audio_process.py:57: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audios.append(librosa.load(path, sr=16000)[0])\n",
      "/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "qwen-vl-utils using torchvision to read video.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_grid_thw: tensor([[165,  16,  16]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"video\", \"video\": \"./test.mp4\"},\n",
    "            {\"type\": \"text\", \"text\": \"Could you detail events during different time segments? Format strictly:\\nFrom xx to xx, event1.\\nFrom xx to xx, event2.\\n...\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "USE_AUDIO_IN_VIDEO = True\n",
    "\n",
    "text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "for msg in conversation:\n",
    "    if msg[\"role\"] == \"user\":\n",
    "        for ele in msg[\"content\"]:\n",
    "            if ele.get(\"type\") == \"video\":\n",
    "                ele[\"fps\"] = 2.0   # 设置 1fps\n",
    "\n",
    "audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "inputs = inputs.to(model.device).to(model.dtype)\n",
    "\n",
    "print(\"video_grid_thw:\", inputs[\"video_grid_thw\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d2f7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system\\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\\nuser\\nCould you detail events during different time segments? Format strictly:\\nFrom xx to xx, event1.\\nFrom xx to xx, event2.\\n...\\nassistant\\nFrom 0.00 to 10.00, the video starts with a black screen and then a white logo appears.\\nFrom 10.00 to 15.00, a man is shown holding a remote control.\\nFrom 15.00 to 20.00, the man is seen wearing a helmet and looking down.\\nFrom 20.00 to 25.00, the man is holding a remote control again.\\nFrom 25.00 to 30.00, the man is holding a remote control and looking at it.\\nFrom 30.00 to 35.00, the man is holding a remote control and looking at it.\\nFrom 35.00 to 40.00, the man is holding a remote control and looking at it.\\nFrom 40.00 to 45.00, the man is holding a remote control and looking at it.\\nFrom 45.00 to 50.00, the man is holding a remote control and looking at it.\\nFrom 50.00 to 55.00, the man is holding a remote control and looking at it.\\nFrom 55.00 to 60.00, the man is holding a remote control and looking at it.\\nFrom 60.00 to 65.00, the man is holding a remote control and looking at it.\\nFrom 65.00 to 70.00, the man is holding a remote control and looking at it.\\nFrom 70.00 to 75.00, the man is holding a remote control and looking at it.\\nFrom 75.00 to 80.00, the man is holding a remote control and looking at it.\\nFrom 80.00 to 85.00, the man is holding a remote control and looking at it.\\nFrom 85.00 to 90.00, the man is holding a remote control and looking at it.\\nFrom 90.00 to 95.00, the man is holding a remote control and looking at it.\\nFrom 95.00 to 100.00, the man is holding a remote control and looking at it.\\nFrom 100.00 to 105.00, the man is holding a remote control and looking at it.\\nFrom 105.00 to 110.00, the man is holding a remote control and looking at it.\\nFrom 110.00 to 115.00, the man is holding a remote control and looking at it.\\nFrom 115.00 to 120.00, the man is holding a remote control and looking at it.\\nFrom 120.00 to 125.00, the man is holding a remote control and looking at it.\\nFrom 125.00 to 130.00, the man is holding a remote control and looking at it.\\nFrom 130.00 to 135.00, the man is holding a remote control and looking at it.\\nFrom 135.00 to 140.00, the man is holding a remote control and looking at it.\\nFrom 140.00 to 145.00, the man is holding a remote control and looking at it.\\nFrom 145.00 to 150.00, the man is holding a remote control and looking at it.\\nFrom 150.00 to 155.00, the man is holding a remote control and looking at it.\\nFrom 155.00 to 160.00, the man is holding a remote control and looking at it.\\nFrom 160.00 to 165.00, the man is holding a remote control and looking at it.\\nFrom 165.00 to 170.00, the man is holding a remote control and looking at it.\\nFrom 170.00 to 175.00, the man is holding a remote control and looking at it.\\nFrom 175.00 to 180.00, the man is holding a remote control and looking at it.\\nFrom 180.00 to 185.00, the man is holding a remote control and looking at it.\\nFrom 185.00 to 190.00, the man is holding a remote']\n"
     ]
    }
   ],
   "source": [
    "text_ids = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO, return_audio=False)\n",
    "\n",
    "text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d347579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42240, 1176])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"pixel_values_videos\"].shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
