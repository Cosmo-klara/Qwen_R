{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb471b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from modelscope import snapshot_download\n",
    "from modelscope import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor, Qwen2_5OmniThinkerForConditionalGeneration\n",
    "import re\n",
    "from transformers.models.qwen2_vl.video_processing_qwen2_vl import Qwen2VLVideoProcessor\n",
    "from qwen_omni_utils import process_mm_info\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from transformers.image_utils import SizeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae29d917",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniStepMemoryTracker:\n",
    "    def __init__(self, log_every=1):\n",
    "        self.step = 0\n",
    "        self.log_every = log_every\n",
    "\n",
    "    def log(self, batch):\n",
    "        if self.step % self.log_every != 0:\n",
    "            self.step += 1\n",
    "            return\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        alloc = torch.cuda.memory_allocated() / 1024**2\n",
    "        reserve = torch.cuda.memory_reserved() / 1024**2\n",
    "        peak = torch.cuda.max_memory_allocated() / 1024**2\n",
    "\n",
    "        # ---------- text ----------\n",
    "        seq_len = batch[\"input_ids\"].shape[1]\n",
    "        label_tokens = (batch[\"labels\"] != -100).sum().item()\n",
    "\n",
    "        # ---------- video ----------\n",
    "        if \"video_grid_thw\" in batch:\n",
    "            t, h, w = batch[\"video_grid_thw\"][0].tolist()\n",
    "            video_tokens = t * h * w\n",
    "        else:\n",
    "            t = h = w = video_tokens = 0\n",
    "\n",
    "        # ---------- audio ----------\n",
    "        if \"input_features\" in batch:\n",
    "            audio_tokens = batch[\"input_features\"].shape[-1]\n",
    "        else:\n",
    "            audio_tokens = 0\n",
    "\n",
    "        print(\n",
    "            f\"[Step {self.step:05d}] \"\n",
    "            f\"seq={seq_len}, label={label_tokens} | \"\n",
    "            f\"video={t}x{h}x{w}={video_tokens} | \"\n",
    "            f\"audio={audio_tokens} | \"\n",
    "            f\"CUDA alloc={alloc:.1f}MB peak={peak:.1f}MB reserve={reserve:.1f}MB\"\n",
    "        )\n",
    "\n",
    "        self.step += 1\n",
    "\n",
    "def replace_time_tokens_with_percentage(text, time_map, duration):\n",
    "\n",
    "    if not time_map or duration is None:\n",
    "        return text\n",
    "\n",
    "    def repl(match):\n",
    "        token = match.group(0)\n",
    "        if token not in time_map:\n",
    "            return token\n",
    "        t = time_map[token]\n",
    "        pct = t / duration * 100.0\n",
    "        return f\"{pct:.1f}%\"\n",
    "\n",
    "    return re.sub(r\"<s\\d+>|<e\\d+>\", repl, text)\n",
    "\n",
    "class OmniVideoConversationDataset(Dataset):\n",
    "    def __init__(self, json_path: str, video_root: str):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            self.raw_data = json.load(f)\n",
    "\n",
    "        self.video_root = video_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.raw_data[idx]\n",
    "\n",
    "        video_id = item[\"id\"]\n",
    "        video_path = os.path.join(self.video_root, f\"{video_id}.mp4\")\n",
    "        audio_path = video_path.replace(\".mp4\", \".wav\")\n",
    "\n",
    "        duration = item.get(\"meta\", {}).get(\"duration\", None)\n",
    "        time_map = item.get(\"meta\", {}).get(\"token\", {})\n",
    "\n",
    "        return {\n",
    "            \"video_path\": video_path,\n",
    "            \"audio_path\": audio_path,\n",
    "            \"conversations\": copy.deepcopy(item[\"conversations\"]),\n",
    "            \"duration\": duration,\n",
    "            \"time_map\": time_map,\n",
    "        }\n",
    "\n",
    "class QwenOmniDataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "        self.tokenizer = processor.tokenizer\n",
    "\n",
    "    def _replace_time_tokens(self, conversations, time_map, duration):\n",
    "        if not time_map or duration is None:\n",
    "            return conversations\n",
    "\n",
    "        def repl(match):\n",
    "            token = match.group(0)\n",
    "            if token not in time_map:\n",
    "                return token\n",
    "            pct = time_map[token] / duration * 100\n",
    "            return f\"{pct:.1f}%\"\n",
    "\n",
    "        for turn in conversations:\n",
    "            turn[\"value\"] = re.sub(r\"<s\\d+>|<e\\d+>\", repl, turn[\"value\"])\n",
    "        return conversations\n",
    "    \n",
    "    def _split_rounds(self, conversations):\n",
    "        rounds = []\n",
    "        cur = []\n",
    "        for turn in conversations:\n",
    "            cur.append(turn)\n",
    "            if turn[\"from\"] == \"gpt\":\n",
    "                rounds.append(cur)\n",
    "                cur = []\n",
    "        return rounds\n",
    "\n",
    "    def _truncate_by_round_with_labels(\n",
    "        self,\n",
    "        base_chat,          # system + video/audio\n",
    "        rounds,             # [[h,g], [h,g], ...]\n",
    "        max_total_tokens    # input + label 最大 token 数\n",
    "    ):\n",
    "        rounds = copy.deepcopy(rounds)\n",
    "\n",
    "        while True:\n",
    "            chat = copy.deepcopy(base_chat)\n",
    "\n",
    "            # 统计 label token\n",
    "            total_tokens = 0\n",
    "            for r in rounds:\n",
    "                for t in r:\n",
    "                    role = \"user\" if t[\"from\"] == \"human\" else \"assistant\"\n",
    "                    chat.append({\n",
    "                        \"role\": role,\n",
    "                        \"content\": [{\"type\": \"text\", \"text\": t[\"value\"]}],\n",
    "                    })\n",
    "\n",
    "            # 用 tokenizer 计算 input token 长度\n",
    "            prompt = self.processor.apply_chat_template(\n",
    "                chat, tokenize=False, add_generation_prompt=False\n",
    "            )\n",
    "            input_tokens = len(self.tokenizer(prompt).input_ids)\n",
    "\n",
    "            # 统计 label token\n",
    "            label_tokens = 0\n",
    "            for r in rounds:\n",
    "                for t in r:\n",
    "                    if t[\"from\"] == \"gpt\":  # 只计算 assistant 输出\n",
    "                        label_tokens += len(self.tokenizer(t[\"value\"]).input_ids)\n",
    "\n",
    "            total_tokens = input_tokens + label_tokens\n",
    "            print(input_tokens, label_tokens)\n",
    "\n",
    "            # 如果总长度符合限制，返回\n",
    "            if total_tokens <= max_total_tokens:\n",
    "                return chat\n",
    "\n",
    "            # ❗ 删除最早的一整轮\n",
    "            rounds = rounds[1:]\n",
    "\n",
    "\n",
    "    def _build_conversation(self, sample):\n",
    "        conversations = self._replace_time_tokens(\n",
    "            sample[\"conversations\"],\n",
    "            sample[\"time_map\"],\n",
    "            sample[\"duration\"],\n",
    "        )\n",
    "\n",
    "        base_chat = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"video\", \"video\": sample[\"video_path\"], \"fps\": 0.5, \"max_frames\": 50},\n",
    "                    {\"type\": \"audio\", \"audio\": sample[\"audio_path\"]},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        rounds = self._split_rounds(conversations)\n",
    "\n",
    "        chat = self._truncate_by_round_with_labels(\n",
    "            base_chat=base_chat,\n",
    "            rounds=rounds,\n",
    "            max_total_tokens=2048  # 或你显存可承受的最大 token 数\n",
    "        )\n",
    "\n",
    "        return chat\n",
    "\n",
    "    def _build_labels(self, input_ids):\n",
    "        labels = input_ids.clone()\n",
    "        labels[:] = -100\n",
    "\n",
    "        im_start = self.tokenizer.convert_tokens_to_ids(\"<|im_start|>\")\n",
    "        im_end = self.tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "        assistant = self.tokenizer.convert_tokens_to_ids(\"assistant\")\n",
    "\n",
    "        i = 0\n",
    "        while i < len(input_ids) - 1:\n",
    "            if input_ids[i] == im_start and input_ids[i + 1] == assistant:\n",
    "                j = i + 3  # skip <|im_start|> assistant \\n\n",
    "                while j < len(input_ids) and input_ids[j] != im_end:\n",
    "                    labels[j] = input_ids[j]\n",
    "                    j += 1\n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "        return labels\n",
    "\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        texts, videos, audios = [], [], []\n",
    "\n",
    "        for sample in features:\n",
    "            conversation = self._build_conversation(sample)\n",
    "\n",
    "            prompt = self.processor.apply_chat_template(\n",
    "                conversation,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False,\n",
    "            )\n",
    "            texts.append(prompt)\n",
    "\n",
    "            audios_, _, videos_ = process_mm_info(\n",
    "                    conversation, use_audio_in_video=False\n",
    "                )\n",
    "\n",
    "            videos.append(videos_[0] if videos_ else None)\n",
    "            audios.append(audios_[0] if audios_ else None)\n",
    "\n",
    "        batch = self.processor(\n",
    "            text=texts,\n",
    "            videos=videos,\n",
    "            audio=audios,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            use_audio_in_video=False,\n",
    "        )\n",
    "\n",
    "        labels = torch.stack([\n",
    "            self._build_labels(ids)\n",
    "            for ids in batch[\"input_ids\"]\n",
    "        ])\n",
    "\n",
    "        label_tokens = (labels != -100).sum().item()\n",
    "        print(\"label tokens:\", label_tokens)\n",
    "\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        if not hasattr(self, \"_debug_printed\"):\n",
    "            # self._debug_printed = True\n",
    "\n",
    "            print(\"\\n========== Omni Batch Debug ==========\")\n",
    "\n",
    "            # ---------- Text ----------\n",
    "            print(\"[Text]\")\n",
    "            print(\"input_ids:\", batch[\"input_ids\"].shape)\n",
    "            print(\"attention_mask:\", batch[\"attention_mask\"].shape)\n",
    "            print(\"labels:\", batch[\"labels\"].shape)\n",
    "            print(\n",
    "                \"label tokens:\",\n",
    "                (batch[\"labels\"] != -100).sum().item()\n",
    "            )\n",
    "\n",
    "            # ---------- Video ----------\n",
    "            if \"pixel_values_videos\" in batch:\n",
    "                pv = batch[\"pixel_values_videos\"]\n",
    "                print(\"\\n[Video]\")\n",
    "                print(\"pixel_values_videos:\", pv.shape)\n",
    "                print(\"dtype:\", pv.dtype)\n",
    "                print(\"video_grid_thw:\", batch.get(\"video_grid_thw\"))\n",
    "\n",
    "                video_mem = pv.numel() * pv.element_size() / 1024**2\n",
    "                print(f\"video tensor size: {video_mem:.2f} MB\")\n",
    "\n",
    "            # ---------- Audio ----------\n",
    "            for k in batch.keys():\n",
    "                if \"audio\" in k or \"input_features\" in k:\n",
    "                    v = batch[k]\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        mem = v.numel() * v.element_size() / 1024**2\n",
    "                        print(\"\\n[Audio]\")\n",
    "                        print(f\"{k}: {v.shape}, {mem:.2f} MB\")\n",
    "\n",
    "            print(\"=====================================\\n\")\n",
    "\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = OmniVideoConversationDataset(\n",
    "    json_path=\"../../LongVALE/data/longvale-sft-bp-7k.json\",\n",
    "    video_root=\"../../LongVALE/raw_videos_train/video_train_7240/\"\n",
    ")\n",
    "\n",
    "\n",
    "model_path = snapshot_download(\n",
    "    'Qwen/Qwen2.5-Omni-3B',\n",
    "    cache_dir=\"../../Qwen/cache/modelscope\"\n",
    ")\n",
    "\n",
    "model = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "class FixedResQwen2VLVideoProcessor(Qwen2VLVideoProcessor):\n",
    "    def _preprocess(\n",
    "        self, videos, do_resize=True, size=None, interpolation=None, **kwargs\n",
    "    ):\n",
    "        # 固定分辨率\n",
    "        fixed_size = SizeDict(height=224, width=224)\n",
    "        for i, video in enumerate(videos):\n",
    "            videos[i] = self.resize(video, size=fixed_size, interpolation=interpolation)\n",
    "        return super()._preprocess(videos, do_resize=False, size=fixed_size, interpolation=interpolation, **kwargs)\n",
    "    \n",
    "video_processor = FixedResQwen2VLVideoProcessor.from_pretrained(model_path)\n",
    "\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(\n",
    "    model_path,\n",
    "    video_processor=video_processor,\n",
    ")\n",
    "\n",
    "\n",
    "# 配置LoRA\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    # task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if (\n",
    "        \"audio_tower\" in name\n",
    "        or \"visual\" in name\n",
    "    ):\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model.config.use_cache = False\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 检查模型是否在训练模式\n",
    "model.train()\n",
    "print(f\"Model is in training mode: {model.training}\")\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./r_models\",\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    fp16_full_eval=False,\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=5,\n",
    "    load_best_model_at_end=False,\n",
    "\n",
    "    torch_compile=True,\n",
    "    torch_compile_backend=\"inductor\",\n",
    "    torch_compile_mode=\"default\",\n",
    "\n",
    "    # 先 pip install liger-kernel\n",
    "\n",
    "    # use_liger_kernel=True,\n",
    "    # liger_kernel_config={\n",
    "    #     \"cross_entropy\": True,\n",
    "    # }\n",
    ")\n",
    "\n",
    "data_collator = QwenOmniDataCollator(processor)\n",
    "\n",
    "class DebugTrainer(Trainer):\n",
    "    def __init__(self, *args, memory_tracker=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.memory_tracker = memory_tracker\n",
    "\n",
    "    def training_step(self, model, inputs, *args, **kwargs):\n",
    "        if self.memory_tracker is not None:\n",
    "            self.memory_tracker.log(inputs)\n",
    "\n",
    "        return super().training_step(model, inputs, *args, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "memory_tracker = OmniStepMemoryTracker(log_every=1)\n",
    "\n",
    "\n",
    "trainer = DebugTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    memory_tracker=memory_tracker,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
