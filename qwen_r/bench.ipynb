{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7d851ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from modelscope import snapshot_download\n",
    "from modelscope import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor, Qwen2_5OmniThinkerForConditionalGeneration\n",
    "import re\n",
    "from transformers.models.qwen2_vl.video_processing_qwen2_vl import Qwen2VLVideoProcessor\n",
    "from qwen_omni_utils import process_mm_info\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from transformers.image_utils import SizeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ff0d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 59\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpct\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<s\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+>|<e\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+>\u001b[39m\u001b[38;5;124m\"\u001b[39m, repl, text)\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mOmniVideoConversationDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, json_path: \u001b[38;5;28mstr\u001b[39m, video_root: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(json_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class OmniStepMemoryTracker:\n",
    "    def __init__(self, log_every=1):\n",
    "        self.step = 0\n",
    "        self.log_every = log_every\n",
    "\n",
    "    def log(self, batch):\n",
    "        if self.step % self.log_every != 0:\n",
    "            self.step += 1\n",
    "            return\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        alloc = torch.cuda.memory_allocated() / 1024**2\n",
    "        reserve = torch.cuda.memory_reserved() / 1024**2\n",
    "        peak = torch.cuda.max_memory_allocated() / 1024**2\n",
    "\n",
    "        # ---------- text ----------\n",
    "        seq_len = batch[\"input_ids\"].shape[1]\n",
    "        label_tokens = (batch[\"labels\"] != -100).sum().item()\n",
    "\n",
    "        # ---------- video ----------\n",
    "        if \"video_grid_thw\" in batch:\n",
    "            t, h, w = batch[\"video_grid_thw\"][0].tolist()\n",
    "            video_tokens = t * h * w\n",
    "        else:\n",
    "            t = h = w = video_tokens = 0\n",
    "\n",
    "        # ---------- audio ----------\n",
    "        if \"input_features\" in batch:\n",
    "            audio_tokens = batch[\"input_features\"].shape[-1]\n",
    "        else:\n",
    "            audio_tokens = 0\n",
    "\n",
    "        print(\n",
    "            f\"[Step {self.step:05d}] \"\n",
    "            f\"seq={seq_len}, label={label_tokens} | \"\n",
    "            f\"video={t}x{h}x{w}={video_tokens} | \"\n",
    "            f\"audio={audio_tokens} | \"\n",
    "            f\"CUDA alloc={alloc:.1f}MB peak={peak:.1f}MB reserve={reserve:.1f}MB\"\n",
    "        )\n",
    "\n",
    "        self.step += 1\n",
    "\n",
    "def replace_time_tokens_with_percentage(text, time_map, duration):\n",
    "\n",
    "    if not time_map or duration is None:\n",
    "        return text\n",
    "\n",
    "    def repl(match):\n",
    "        token = match.group(0)\n",
    "        if token not in time_map:\n",
    "            return token\n",
    "        t = time_map[token]\n",
    "        pct = t / duration * 100.0\n",
    "        return f\"{pct:.1f}%\"\n",
    "\n",
    "    return re.sub(r\"<s\\d+>|<e\\d+>\", repl, text)\n",
    "\n",
    "class OmniVideoConversationDataset(Dataset):\n",
    "    def __init__(self, json_path: str, video_root: str):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            self.raw_data = json.load(f)\n",
    "\n",
    "        self.video_root = video_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.raw_data[idx]\n",
    "\n",
    "        video_id = item[\"id\"]\n",
    "        video_path = os.path.join(self.video_root, f\"{video_id}.mp4\")\n",
    "        audio_path = video_path.replace(\".mp4\", \".wav\")\n",
    "\n",
    "        duration = item.get(\"meta\", {}).get(\"duration\", None)\n",
    "        time_map = item.get(\"meta\", {}).get(\"token\", {})\n",
    "\n",
    "        return {\n",
    "            \"video_path\": video_path,\n",
    "            \"audio_path\": audio_path,\n",
    "            \"conversations\": copy.deepcopy(item[\"conversations\"]),\n",
    "            \"duration\": duration,\n",
    "            \"time_map\": time_map,\n",
    "        }\n",
    "\n",
    "class QwenOmniDataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "        self.tokenizer = processor.tokenizer\n",
    "\n",
    "    def _replace_time_tokens(self, conversations, time_map, duration):\n",
    "        if not time_map or duration is None:\n",
    "            return conversations\n",
    "\n",
    "        def repl(match):\n",
    "            token = match.group(0)\n",
    "            if token not in time_map:\n",
    "                return token\n",
    "            pct = time_map[token] / duration * 100\n",
    "            return f\"{pct:.1f}%\"\n",
    "\n",
    "        for turn in conversations:\n",
    "            turn[\"value\"] = re.sub(r\"<s\\d+>|<e\\d+>\", repl, turn[\"value\"])\n",
    "        return conversations\n",
    "    \n",
    "    def _split_rounds(self, conversations):\n",
    "        rounds = []\n",
    "        cur = []\n",
    "        for turn in conversations:\n",
    "            cur.append(turn)\n",
    "            if turn[\"from\"] == \"gpt\":\n",
    "                rounds.append(cur)\n",
    "                cur = []\n",
    "        return rounds\n",
    "\n",
    "    def _truncate_by_round_with_labels(\n",
    "        self,\n",
    "        base_chat,          # system + video/audio\n",
    "        rounds,             # [[h,g], [h,g], ...]\n",
    "        max_total_tokens    # input + label 最大 token 数\n",
    "    ):\n",
    "        rounds = copy.deepcopy(rounds)\n",
    "\n",
    "        while True:\n",
    "            chat = copy.deepcopy(base_chat)\n",
    "\n",
    "            # 统计 label token\n",
    "            total_tokens = 0\n",
    "            for r in rounds:\n",
    "                for t in r:\n",
    "                    role = \"user\" if t[\"from\"] == \"human\" else \"assistant\"\n",
    "                    chat.append({\n",
    "                        \"role\": role,\n",
    "                        \"content\": [{\"type\": \"text\", \"text\": t[\"value\"]}],\n",
    "                    })\n",
    "\n",
    "            # 用 tokenizer 计算 input token 长度\n",
    "            prompt = self.processor.apply_chat_template(\n",
    "                chat, tokenize=False, add_generation_prompt=False\n",
    "            )\n",
    "            input_tokens = len(self.tokenizer(prompt).input_ids)\n",
    "\n",
    "            # 统计 label token\n",
    "            label_tokens = 0\n",
    "            for r in rounds:\n",
    "                for t in r:\n",
    "                    if t[\"from\"] == \"gpt\":  # 只计算 assistant 输出\n",
    "                        label_tokens += len(self.tokenizer(t[\"value\"]).input_ids)\n",
    "\n",
    "            total_tokens = input_tokens + label_tokens\n",
    "            print(input_tokens, label_tokens)\n",
    "\n",
    "            # 如果总长度符合限制，返回\n",
    "            if total_tokens <= max_total_tokens:\n",
    "                return chat\n",
    "\n",
    "            # ❗ 删除最早的一整轮\n",
    "            rounds = rounds[1:]\n",
    "\n",
    "\n",
    "    def _build_conversation(self, sample):\n",
    "        conversations = self._replace_time_tokens(\n",
    "            sample[\"conversations\"],\n",
    "            sample[\"time_map\"],\n",
    "            sample[\"duration\"],\n",
    "        )\n",
    "\n",
    "        base_chat = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"video\", \"video\": sample[\"video_path\"], \"fps\": 0.5, \"max_frames\": 50},\n",
    "                    {\"type\": \"audio\", \"audio\": sample[\"audio_path\"]},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        rounds = self._split_rounds(conversations)\n",
    "\n",
    "        chat = self._truncate_by_round_with_labels(\n",
    "            base_chat=base_chat,\n",
    "            rounds=rounds,\n",
    "            max_total_tokens=2048  # 或你显存可承受的最大 token 数\n",
    "        )\n",
    "\n",
    "        return chat\n",
    "\n",
    "    def _build_labels(self, input_ids):\n",
    "        labels = input_ids.clone()\n",
    "        labels[:] = -100\n",
    "\n",
    "        im_start = self.tokenizer.convert_tokens_to_ids(\"<|im_start|>\")\n",
    "        im_end = self.tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "        assistant = self.tokenizer.convert_tokens_to_ids(\"assistant\")\n",
    "\n",
    "        i = 0\n",
    "        while i < len(input_ids) - 1:\n",
    "            if input_ids[i] == im_start and input_ids[i + 1] == assistant:\n",
    "                j = i + 3  # skip <|im_start|> assistant \\n\n",
    "                while j < len(input_ids) and input_ids[j] != im_end:\n",
    "                    labels[j] = input_ids[j]\n",
    "                    j += 1\n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "        return labels\n",
    "\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        texts, videos, audios = [], [], []\n",
    "\n",
    "        for sample in features:\n",
    "            conversation = self._build_conversation(sample)\n",
    "\n",
    "            prompt = self.processor.apply_chat_template(\n",
    "                conversation,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False,\n",
    "            )\n",
    "            texts.append(prompt)\n",
    "\n",
    "            audios_, _, videos_ = process_mm_info(\n",
    "                    conversation, use_audio_in_video=False\n",
    "                )\n",
    "\n",
    "            videos.append(videos_[0] if videos_ else None)\n",
    "            audios.append(audios_[0] if audios_ else None)\n",
    "\n",
    "        batch = self.processor(\n",
    "            text=texts,\n",
    "            videos=videos,\n",
    "            audio=audios,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            use_audio_in_video=False,\n",
    "        )\n",
    "\n",
    "        labels = torch.stack([\n",
    "            self._build_labels(ids)\n",
    "            for ids in batch[\"input_ids\"]\n",
    "        ])\n",
    "\n",
    "        label_tokens = (labels != -100).sum().item()\n",
    "        print(\"label tokens:\", label_tokens)\n",
    "\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        if not hasattr(self, \"_debug_printed\"):\n",
    "            # self._debug_printed = True\n",
    "\n",
    "            print(\"\\n========== Omni Batch Debug ==========\")\n",
    "\n",
    "            # ---------- Text ----------\n",
    "            print(\"[Text]\")\n",
    "            print(\"input_ids:\", batch[\"input_ids\"].shape)\n",
    "            print(\"attention_mask:\", batch[\"attention_mask\"].shape)\n",
    "            print(\"labels:\", batch[\"labels\"].shape)\n",
    "            print(\n",
    "                \"label tokens:\",\n",
    "                (batch[\"labels\"] != -100).sum().item()\n",
    "            )\n",
    "\n",
    "            # ---------- Video ----------\n",
    "            if \"pixel_values_videos\" in batch:\n",
    "                pv = batch[\"pixel_values_videos\"]\n",
    "                print(\"\\n[Video]\")\n",
    "                print(\"pixel_values_videos:\", pv.shape)\n",
    "                print(\"dtype:\", pv.dtype)\n",
    "                print(\"video_grid_thw:\", batch.get(\"video_grid_thw\"))\n",
    "\n",
    "                video_mem = pv.numel() * pv.element_size() / 1024**2\n",
    "                print(f\"video tensor size: {video_mem:.2f} MB\")\n",
    "\n",
    "            # ---------- Audio ----------\n",
    "            for k in batch.keys():\n",
    "                if \"audio\" in k or \"input_features\" in k:\n",
    "                    v = batch[k]\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        mem = v.numel() * v.element_size() / 1024**2\n",
    "                        print(\"\\n[Audio]\")\n",
    "                        print(f\"{k}: {v.shape}, {mem:.2f} MB\")\n",
    "\n",
    "            print(\"=====================================\\n\")\n",
    "\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = OmniVideoConversationDataset(\n",
    "    json_path=\"../../LongVALE/data/longvale-sft-bp-7k.json\",\n",
    "    video_root=\"../../LongVALE/raw_videos_train/video_train_7240/\"\n",
    ")\n",
    "\n",
    "\n",
    "model_path = snapshot_download(\n",
    "    'Qwen/Qwen2.5-Omni-3B',\n",
    "    cache_dir=\"../../Qwen/cache/modelscope\"\n",
    ")\n",
    "\n",
    "model = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "class FixedResQwen2VLVideoProcessor(Qwen2VLVideoProcessor):\n",
    "    def _preprocess(\n",
    "        self, videos, do_resize=True, size=None, interpolation=None, **kwargs\n",
    "    ):\n",
    "        # 固定分辨率\n",
    "        fixed_size = SizeDict(height=224, width=224)\n",
    "        for i, video in enumerate(videos):\n",
    "            videos[i] = self.resize(video, size=fixed_size, interpolation=interpolation)\n",
    "        return super()._preprocess(videos, do_resize=False, size=fixed_size, interpolation=interpolation, **kwargs)\n",
    "    \n",
    "video_processor = FixedResQwen2VLVideoProcessor.from_pretrained(model_path)\n",
    "\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(\n",
    "    model_path,\n",
    "    video_processor=video_processor,\n",
    ")\n",
    "\n",
    "\n",
    "# 配置LoRA\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    # task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if (\n",
    "        \"audio_tower\" in name\n",
    "        or \"visual\" in name\n",
    "    ):\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model.config.use_cache = False\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 检查模型是否在训练模式\n",
    "model.train()\n",
    "print(f\"Model is in training mode: {model.training}\")\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./r_models\",\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    fp16_full_eval=False,\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=5,\n",
    "    load_best_model_at_end=False,\n",
    ")\n",
    "\n",
    "data_collator = QwenOmniDataCollator(processor)\n",
    "\n",
    "class DebugTrainer(Trainer):\n",
    "    def __init__(self, *args, memory_tracker=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.memory_tracker = memory_tracker\n",
    "\n",
    "    def training_step(self, model, inputs, *args, **kwargs):\n",
    "        if self.memory_tracker is not None:\n",
    "            self.memory_tracker.log(inputs)\n",
    "\n",
    "        return super().training_step(model, inputs, *args, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "memory_tracker = OmniStepMemoryTracker(log_every=1)\n",
    "\n",
    "\n",
    "trainer = DebugTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    memory_tracker=memory_tracker,\n",
    ")\n",
    "\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a4f48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 151643\n",
      "number of tokens: 151665\n",
      "32768\n"
     ]
    }
   ],
   "source": [
    "print(\"vocab size:\", processor.tokenizer.vocab_size)\n",
    "print(\"number of tokens:\", len(processor.tokenizer))\n",
    "print(processor.tokenizer.model_max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde099e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1590 547\n",
      "1520 514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qwen-vl-utils using decord to read video.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label tokens: 514\n",
      "max label length: 514\n",
      "\n",
      "========== Omni Batch Debug ==========\n",
      "[Text]\n",
      "input_ids: torch.Size([1, 10618])\n",
      "attention_mask: torch.Size([1, 10618])\n",
      "labels: torch.Size([1, 10618])\n",
      "label tokens: 514\n",
      "\n",
      "[Video]\n",
      "pixel_values_videos: torch.Size([6400, 1176])\n",
      "dtype: torch.float32\n",
      "video_grid_thw: tensor([[25, 16, 16]])\n",
      "video tensor size: 28.71 MB\n",
      "\n",
      "[Audio]\n",
      "input_features: torch.Size([1, 128, 30000]), 14.65 MB\n",
      "=====================================\n",
      "\n",
      "496 210\n",
      "label tokens: 210\n",
      "max label length: 210\n",
      "\n",
      "========== Omni Batch Debug ==========\n",
      "[Text]\n",
      "input_ids: torch.Size([1, 2188])\n",
      "attention_mask: torch.Size([1, 2188])\n",
      "labels: torch.Size([1, 2188])\n",
      "label tokens: 210\n",
      "\n",
      "[Video]\n",
      "pixel_values_videos: torch.Size([2560, 1176])\n",
      "dtype: torch.float32\n",
      "video_grid_thw: tensor([[10, 16, 16]])\n",
      "video tensor size: 11.48 MB\n",
      "\n",
      "[Audio]\n",
      "input_features: torch.Size([1, 128, 30000]), 14.65 MB\n",
      "=====================================\n",
      "\n",
      "[Step 00000] seq=10618, label=514 | video=25x16x16=6400 | audio=30000 | CUDA alloc=5559.8MB peak=5559.9MB reserve=5568.0MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='14480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   18/14480 01:43 < 25:54:40, 0.16 it/s, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.731600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.972000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.495800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1072 993\n",
      "label tokens: 993\n",
      "max label length: 993\n",
      "\n",
      "========== Omni Batch Debug ==========\n",
      "[Text]\n",
      "input_ids: torch.Size([1, 5595])\n",
      "attention_mask: torch.Size([1, 5595])\n",
      "labels: torch.Size([1, 5595])\n",
      "label tokens: 993\n",
      "\n",
      "[Video]\n",
      "pixel_values_videos: torch.Size([6400, 1176])\n",
      "dtype: torch.float32\n",
      "video_grid_thw: tensor([[25, 16, 16]])\n",
      "video tensor size: 28.71 MB\n",
      "\n",
      "[Audio]\n",
      "input_features: torch.Size([1, 128, 30000]), 14.65 MB\n",
      "=====================================\n",
      "\n",
      "[Step 00001] seq=2188, label=210 | video=10x16x16=2560 | audio=30000 | CUDA alloc=5634.7MB peak=15545.8MB reserve=15842.0MB\n",
      "914 314\n",
      "label tokens: 314\n",
      "max label length: 314\n",
      "\n",
      "========== Omni Batch Debug ==========\n",
      "[Text]\n",
      "input_ids: torch.Size([1, 7021])\n",
      "attention_mask: torch.Size([1, 7021])\n",
      "labels: torch.Size([1, 7021])\n",
      "label tokens: 314\n",
      "\n",
      "[Video]\n",
      "pixel_values_videos: torch.Size([6400, 1176])\n",
      "dtype: torch.float32\n",
      "video_grid_thw: tensor([[25, 16, 16]])\n",
      "video tensor size: 28.71 MB\n",
      "\n",
      "[Audio]\n",
      "input_features: torch.Size([1, 128, 30000]), 14.65 MB\n",
      "=====================================\n",
      "\n",
      "[Step 00002] seq=5595, label=993 | video=25x16x16=6400 | audio=30000 | CUDA alloc=5652.0MB peak=15545.8MB reserve=15842.0MB\n",
      "473 136\n",
      "label tokens: 136\n",
      "max label length: 136\n",
      "\n",
      "========== Omni Batch Debug ==========\n",
      "[Text]\n",
      "input_ids: torch.Size([1, 6892])\n",
      "attention_mask: torch.Size([1, 6892])\n",
      "labels: torch.Size([1, 6892])\n",
      "label tokens: 136\n",
      "\n",
      "[Video]\n",
      "pixel_values_videos: torch.Size([6400, 1176])\n",
      "dtype: torch.float32\n",
      "video_grid_thw: tensor([[25, 16, 16]])\n",
      "video tensor size: 28.71 MB\n",
      "\n",
      "[Audio]\n",
      "input_features: torch.Size([1, 128, 30000]), 14.65 MB\n",
      "=====================================\n",
      "\n",
      "[Step 00003] seq=7021, label=314 | video=25x16x16=6400 | audio=30000 | CUDA alloc=5652.0MB peak=15545.8MB reserve=15842.0MB\n",
      "277 82\n",
      "label tokens: 82\n",
      "max label length: 82\n",
      "\n",
      "========== Omni Batch Debug ==========\n",
      "[Text]\n",
      "input_ids: torch.Size([1, 7297])\n",
      "attention_mask: torch.Size([1, 7297])\n",
      "labels: torch.Size([1, 7297])\n",
      "label tokens: 82\n",
      "\n",
      "[Video]\n",
      "pixel_values_videos: torch.Size([6400, 1176])\n",
      "dtype: torch.float32\n",
      "video_grid_thw: tensor([[25, 16, 16]])\n",
      "video tensor size: 28.71 MB\n",
      "\n",
      "[Audio]\n",
      "input_features: torch.Size([1, 128, 30000]), 14.65 MB\n",
      "=====================================\n",
      "\n",
      "[Step 00004] seq=6892, label=136 | video=25x16x16=6400 | audio=30000 | CUDA alloc=5652.0MB peak=15545.8MB reserve=15846.0MB\n",
      "424 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x13a1d2c0] mmco: unref short failure\n",
      "[h264 @ 0x13a1d2c0] mmco: unref short failure\n",
      "[h264 @ 0x13a1d2c0] mmco: unref short failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label tokens: 146\n",
      "max label length: 146\n",
      "\n",
      "========== Omni Batch Debug ==========\n",
      "[Text]\n",
      "input_ids: torch.Size([1, 5058])\n",
      "attention_mask: torch.Size([1, 5058])\n",
      "labels: torch.Size([1, 5058])\n",
      "label tokens: 146\n",
      "\n",
      "[Video]\n",
      "pixel_values_videos: torch.Size([6400, 1176])\n",
      "dtype: torch.float32\n",
      "video_grid_thw: tensor([[25, 16, 16]])\n",
      "video tensor size: 28.71 MB\n",
      "\n",
      "[Audio]\n",
      "input_features: torch.Size([1, 128, 30000]), 14.65 MB\n",
      "=====================================\n",
      "\n",
      "[Step 00005] seq=7297, label=82 | video=25x16x16=6400 | audio=30000 | CUDA alloc=5652.0MB peak=15545.8MB reserve=15846.0MB\n",
      "1906 928\n",
      "1811 868\n",
      "1638 726\n",
      "1569 689\n",
      "1504 654\n",
      "1434 615\n",
      "1360 572\n",
      "label tokens: 572\n",
      "max label length: 572\n",
      "\n",
      "========== Omni Batch Debug ==========\n",
      "[Text]\n",
      "input_ids: torch.Size([1, 7099])\n",
      "attention_mask: torch.Size([1, 7099])\n",
      "labels: torch.Size([1, 7099])\n",
      "label tokens: 572\n",
      "\n",
      "[Video]\n",
      "pixel_values_videos: torch.Size([6400, 1176])\n",
      "dtype: torch.float32\n",
      "video_grid_thw: tensor([[25, 16, 16]])\n",
      "video tensor size: 28.71 MB\n",
      "\n",
      "[Audio]\n",
      "input_features: torch.Size([1, 128, 30000]), 14.65 MB\n",
      "=====================================\n",
      "\n",
      "[Step 00006] seq=5058, label=146 | video=25x16x16=6400 | audio=30000 | CUDA alloc=5652.0MB peak=15545.8MB reserve=15846.0MB\n",
      "524 182\n",
      "label tokens: 182\n",
      "max label length: 182\n",
      "\n",
      "========== Omni Batch Debug ==========\n",
      "[Text]\n",
      "input_ids: torch.Size([1, 5146])\n",
      "attention_mask: torch.Size([1, 5146])\n",
      "labels: torch.Size([1, 5146])\n",
      "label tokens: 182\n",
      "\n",
      "[Video]\n",
      "pixel_values_videos: torch.Size([6400, 1176])\n",
      "dtype: torch.float32\n",
      "video_grid_thw: tensor([[25, 16, 16]])\n",
      "video tensor size: 28.71 MB\n",
      "\n",
      "[Audio]\n",
      "input_features: torch.Size([1, 128, 30000]), 14.65 MB\n",
      "=====================================\n",
      "\n",
      "[Step 00007] seq=7099, label=572 | video=25x16x16=6400 | audio=30000 | CUDA alloc=5652.0MB peak=15545.8MB reserve=15846.0MB\n",
      "1317 1237\n",
      "label tokens: 1237\n",
      "max label length: 1237\n",
      "\n",
      "========== Omni Batch Debug ==========\n",
      "[Text]\n",
      "input_ids: torch.Size([1, 10415])\n",
      "attention_mask: torch.Size([1, 10415])\n",
      "labels: torch.Size([1, 10415])\n",
      "label tokens: 1237\n",
      "\n",
      "[Video]\n",
      "pixel_values_videos: torch.Size([6400, 1176])\n",
      "dtype: torch.float32\n",
      "video_grid_thw: tensor([[25, 16, 16]])\n",
      "video tensor size: 28.71 MB\n",
      "\n",
      "[Audio]\n",
      "input_features: torch.Size([1, 128, 30000]), 14.65 MB\n",
      "=====================================\n",
      "\n",
      "[Step 00008] seq=5146, label=182 | video=25x16x16=6400 | audio=30000 | CUDA alloc=5652.0MB peak=15545.8MB reserve=15846.0MB\n",
      "868 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x13838a80] mmco: unref short failure\n",
      "[h264 @ 0x13838a80] mmco: unref short failure\n",
      "[h264 @ 0x13838a80] mmco: unref short failure\n",
      "[h264 @ 0x13838a80] mmco: unref short failure\n",
      "[h264 @ 0x13838a80] mmco: unref short failure\n",
      "[h264 @ 0x13838a80] mmco: unref short failure\n",
      "[h264 @ 0x13838a80] mmco: unref short failure\n",
      "[h264 @ 0x13838a80] mmco: unref short failure\n",
      "[h264 @ 0x13838a80] mmco: unref short failure\n",
      "[h264 @ 0x13838a80] mmco: unref short failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label tokens: 250\n",
      "max label length: 250\n",
      "\n",
      "========== Omni Batch Debug ==========\n",
      "[Text]\n",
      "input_ids: torch.Size([1, 6295])\n",
      "attention_mask: torch.Size([1, 6295])\n",
      "labels: torch.Size([1, 6295])\n",
      "label tokens: 250\n",
      "\n",
      "[Video]\n",
      "pixel_values_videos: torch.Size([6400, 1176])\n",
      "dtype: torch.float32\n",
      "video_grid_thw: tensor([[25, 16, 16]])\n",
      "video tensor size: 28.71 MB\n",
      "\n",
      "[Audio]\n",
      "input_features: torch.Size([1, 128, 30000]), 14.65 MB\n",
      "=====================================\n",
      "\n",
      "[Step 00009] seq=10415, label=1237 | video=25x16x16=6400 | audio=30000 | CUDA alloc=5652.1MB peak=15545.8MB reserve=15846.0MB\n",
      "343 111\n",
      "label tokens: 111\n",
      "max label length: 111\n",
      "\n",
      "========== Omni Batch Debug ==========\n",
      "[Text]\n",
      "input_ids: torch.Size([1, 2054])\n",
      "attention_mask: torch.Size([1, 2054])\n",
      "labels: torch.Size([1, 2054])\n",
      "label tokens: 111\n",
      "\n",
      "[Video]\n",
      "pixel_values_videos: torch.Size([2560, 1176])\n",
      "dtype: torch.float32\n",
      "video_grid_thw: tensor([[10, 16, 16]])\n",
      "video tensor size: 11.48 MB\n",
      "\n",
      "[Audio]\n",
      "input_features: torch.Size([1, 128, 30000]), 14.65 MB\n",
      "=====================================\n",
      "\n",
      "[Step 00010] seq=6295, label=250 | video=25x16x16=6400 | audio=30000 | CUDA alloc=5652.0MB peak=15545.8MB reserve=15846.0MB\n",
      "1420 461\n",
      "label tokens: 461\n",
      "max label length: 461\n",
      "\n",
      "========== Omni Batch Debug ==========\n",
      "[Text]\n",
      "input_ids: torch.Size([1, 10288])\n",
      "attention_mask: torch.Size([1, 10288])\n",
      "labels: torch.Size([1, 10288])\n",
      "label tokens: 461\n",
      "\n",
      "[Video]\n",
      "pixel_values_videos: torch.Size([6400, 1176])\n",
      "dtype: torch.float32\n",
      "video_grid_thw: tensor([[25, 16, 16]])\n",
      "video tensor size: 28.71 MB\n",
      "\n",
      "[Audio]\n",
      "input_features: torch.Size([1, 128, 30000]), 14.65 MB\n",
      "=====================================\n",
      "\n",
      "[Step 00011] seq=2054, label=111 | video=10x16x16=2560 | audio=30000 | CUDA alloc=5634.7MB peak=15545.8MB reserve=15846.0MB\n",
      "145 58\n",
      "label tokens: 58\n",
      "max label length: 58\n",
      "\n",
      "========== Omni Batch Debug ==========\n",
      "[Text]\n",
      "input_ids: torch.Size([1, 2681])\n",
      "attention_mask: torch.Size([1, 2681])\n",
      "labels: torch.Size([1, 2681])\n",
      "label tokens: 58\n",
      "\n",
      "[Video]\n",
      "pixel_values_videos: torch.Size([3840, 1176])\n",
      "dtype: torch.float32\n",
      "video_grid_thw: tensor([[15, 16, 16]])\n",
      "video tensor size: 17.23 MB\n",
      "\n",
      "[Audio]\n",
      "input_features: torch.Size([1, 128, 30000]), 14.65 MB\n",
      "=====================================\n",
      "\n",
      "[Step 00012] seq=10288, label=461 | video=25x16x16=6400 | audio=30000 | CUDA alloc=5652.1MB peak=15545.8MB reserve=15846.0MB\n",
      "524 148\n",
      "label tokens: 148\n",
      "max label length: 148\n",
      "\n",
      "========== Omni Batch Debug ==========\n",
      "[Text]\n",
      "input_ids: torch.Size([1, 3984])\n",
      "attention_mask: torch.Size([1, 3984])\n",
      "labels: torch.Size([1, 3984])\n",
      "label tokens: 148\n",
      "\n",
      "[Video]\n",
      "pixel_values_videos: torch.Size([5376, 1176])\n",
      "dtype: torch.float32\n",
      "video_grid_thw: tensor([[21, 16, 16]])\n",
      "video tensor size: 24.12 MB\n",
      "\n",
      "[Audio]\n",
      "input_features: torch.Size([1, 128, 30000]), 14.65 MB\n",
      "=====================================\n",
      "\n",
      "[Step 00013] seq=2681, label=58 | video=15x16x16=3840 | audio=30000 | CUDA alloc=5640.4MB peak=15545.8MB reserve=15846.0MB\n",
      "4795 1723\n",
      "4723 1709\n",
      "4614 1633\n",
      "4550 1601\n",
      "4490 1574\n",
      "4412 1560\n",
      "4341 1523\n",
      "4263 1509\n",
      "4188 1467\n",
      "4114 1453\n",
      "4037 1409\n",
      "3970 1395\n",
      "3902 1381\n",
      "3835 1367\n",
      "3759 1324\n",
      "3695 1310\n",
      "3623 1296\n",
      "3549 1255\n",
      "3460 1241\n",
      "3379 1227\n",
      "3291 1215\n",
      "3214 1170\n",
      "3135 1156\n",
      "3051 1142\n",
      "2989 1113\n",
      "2930 1099\n",
      "2855 1085\n",
      "2788 1050\n",
      "2714 1008\n",
      "2630 994\n",
      "2556 953\n",
      "2482 939\n",
      "2406 925\n",
      "2336 911\n",
      "2260 867\n",
      "2197 853\n",
      "2104 791\n",
      "2039 777\n",
      "1970 763\n",
      "1883 749\n",
      "1810 708\n",
      "1723 653\n",
      "1646 639\n",
      "1560 587\n",
      "1488 547\n",
      "label tokens: 547\n",
      "max label length: 547\n",
      "\n",
      "========== Omni Batch Debug ==========\n",
      "[Text]\n",
      "input_ids: torch.Size([1, 10586])\n",
      "attention_mask: torch.Size([1, 10586])\n",
      "labels: torch.Size([1, 10586])\n",
      "label tokens: 547\n",
      "\n",
      "[Video]\n",
      "pixel_values_videos: torch.Size([6400, 1176])\n",
      "dtype: torch.float32\n",
      "video_grid_thw: tensor([[25, 16, 16]])\n",
      "video tensor size: 28.71 MB\n",
      "\n",
      "[Audio]\n",
      "input_features: torch.Size([1, 128, 30000]), 14.65 MB\n",
      "=====================================\n",
      "\n",
      "[Step 00014] seq=3984, label=148 | video=21x16x16=5376 | audio=30000 | CUDA alloc=5647.3MB peak=15545.8MB reserve=15846.0MB\n",
      "302 121\n",
      "label tokens: 121\n",
      "max label length: 121\n",
      "\n",
      "========== Omni Batch Debug ==========\n",
      "[Text]\n",
      "input_ids: torch.Size([1, 7949])\n",
      "attention_mask: torch.Size([1, 7949])\n",
      "labels: torch.Size([1, 7949])\n",
      "label tokens: 121\n",
      "\n",
      "[Video]\n",
      "pixel_values_videos: torch.Size([6400, 1176])\n",
      "dtype: torch.float32\n",
      "video_grid_thw: tensor([[25, 16, 16]])\n",
      "video tensor size: 28.71 MB\n",
      "\n",
      "[Audio]\n",
      "input_features: torch.Size([1, 128, 30000]), 14.65 MB\n",
      "=====================================\n",
      "\n",
      "[Step 00015] seq=10586, label=547 | video=25x16x16=6400 | audio=30000 | CUDA alloc=5652.1MB peak=15545.8MB reserve=15846.0MB\n",
      "503 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x15bf1f00] mmco: unref short failure\n",
      "[h264 @ 0x15bf1f00] mmco: unref short failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label tokens: 146\n",
      "max label length: 146\n",
      "\n",
      "========== Omni Batch Debug ==========\n",
      "[Text]\n",
      "input_ids: torch.Size([1, 4532])\n",
      "attention_mask: torch.Size([1, 4532])\n",
      "labels: torch.Size([1, 4532])\n",
      "label tokens: 146\n",
      "\n",
      "[Video]\n",
      "pixel_values_videos: torch.Size([6144, 1176])\n",
      "dtype: torch.float32\n",
      "video_grid_thw: tensor([[24, 16, 16]])\n",
      "video tensor size: 27.56 MB\n",
      "\n",
      "[Audio]\n",
      "input_features: torch.Size([1, 128, 30000]), 14.65 MB\n",
      "=====================================\n",
      "\n",
      "[Step 00016] seq=7949, label=121 | video=25x16x16=6400 | audio=30000 | CUDA alloc=5652.0MB peak=15599.8MB reserve=15866.0MB\n",
      "1521 1441\n",
      "label tokens: 1441\n",
      "max label length: 1441\n",
      "\n",
      "========== Omni Batch Debug ==========\n",
      "[Text]\n",
      "input_ids: torch.Size([1, 8586])\n",
      "attention_mask: torch.Size([1, 8586])\n",
      "labels: torch.Size([1, 8586])\n",
      "label tokens: 1441\n",
      "\n",
      "[Video]\n",
      "pixel_values_videos: torch.Size([6400, 1176])\n",
      "dtype: torch.float32\n",
      "video_grid_thw: tensor([[25, 16, 16]])\n",
      "video tensor size: 28.71 MB\n",
      "\n",
      "[Audio]\n",
      "input_features: torch.Size([1, 128, 30000]), 14.65 MB\n",
      "=====================================\n",
      "\n",
      "[Step 00017] seq=4532, label=146 | video=24x16x16=6144 | audio=30000 | CUDA alloc=5650.8MB peak=15599.8MB reserve=15866.0MB\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import signature\n",
    "print(signature(Qwen2_5OmniForConditionalGeneration.forward))\n",
    "print(signature(Qwen2_5OmniThinkerForConditionalGeneration.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67b27ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_lora_gradients(model):\n",
    "    \"\"\"验证LoRA梯度流\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # 创建测试数据\n",
    "    test_inputs = {\n",
    "        'input_ids': torch.randint(0, 1000, (1, 10)).cuda(),\n",
    "        'attention_mask': torch.ones(1, 10).cuda(),\n",
    "        'labels': torch.randint(0, 1000, (1, 10)).cuda(),\n",
    "    }\n",
    "    \n",
    "    # 前向传播\n",
    "    outputs = model(**test_inputs)\n",
    "    loss = outputs.loss\n",
    "    \n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "    print(f\"Loss requires_grad: {loss.requires_grad}\")\n",
    "    \n",
    "    if loss.requires_grad:\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 检查梯度\n",
    "        gradients_found = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                gradients_found = True\n",
    "                grad_norm = param.grad.norm().item()\n",
    "                if 'lora' in name:\n",
    "                    print(f\"  ✓ LoRA梯度: {name} | norm={grad_norm:.6f}\")\n",
    "        \n",
    "        if not gradients_found:\n",
    "            print(\"  ⚠ 没有找到梯度\")\n",
    "    else:\n",
    "        print(\"  ✗ Loss没有requires_grad\")\n",
    "\n",
    "verify_lora_gradients(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c6c712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取训练集第一个样本\n",
    "sample = train_dataset[11]\n",
    "\n",
    "# 通过 data_collator 生成 batch\n",
    "batch = data_collator([sample])\n",
    "\n",
    "labels = batch[\"labels\"]       # [batch_size, seq_len]\n",
    "input_ids = batch[\"input_ids\"] # [batch_size, seq_len]\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "for i in range(labels.shape[0]):\n",
    "    effective_ids = []\n",
    "    ignored_ids = []\n",
    "    \n",
    "    for j in range(labels.shape[1]):\n",
    "        token_id = input_ids[i, j].item()\n",
    "        label = labels[i, j].item()\n",
    "        if label != -100:\n",
    "            effective_ids.append(token_id)\n",
    "        else:\n",
    "            ignored_ids.append(token_id)\n",
    "    \n",
    "    effective_text = tokenizer.decode(effective_ids)\n",
    "    ignored_text = tokenizer.decode(ignored_ids)\n",
    "    \n",
    "    print(f\"=== Sample {i} ===\")\n",
    "    print(\"有效 token 拼接文本:\")\n",
    "    print(effective_text)\n",
    "    print(\"无效 token 拼接文本:\")\n",
    "    print(ignored_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b175e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.synchronize()\n",
    "print(\n",
    "    f\"Max memory allocated: {torch.cuda.max_memory_allocated() / 1024**2:.2f} MB\"\n",
    ")\n",
    "print(\n",
    "    f\"Max memory reserved: {torch.cuda.max_memory_reserved() / 1024**2:.2f} MB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2834ad",
   "metadata": {},
   "source": [
    "=== Sample 0 ===\n",
    "有效 token 拼接文本:\n",
    "From <s4> to <e4>.\n",
    "\n",
    "这里有个问题，<s4> 转化成什么时间\n",
    "- 用真实时间？\n",
    "    因为 Qwen 实际上能学习到真实的时间，但是评估的时候要算成相对时间吗？还是直接和数据集中的计算？但是这样和 LongVale 算是不是不太公平\n",
    "- 和 LongVale 一样用相对时间百分比\n",
    "    感觉这个是对应的固定 100 帧，如果 Qwen 也固定一百帧的话（不太确定这样能不能对应成绝对时间？），而且固定 100帧 目前显存也不够\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f9df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== 进行前向传播测试 ===\")\n",
    "with torch.set_grad_enabled(True):\n",
    "\n",
    "    # sample = next(iter(train_dataset))\n",
    "    sample = train_dataset[13]\n",
    "    batch = data_collator([sample])\n",
    "    \n",
    "\n",
    "    device = model.device\n",
    "    batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "    \n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    \n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Loss requires_grad: {loss.requires_grad}\")\n",
    "    \n",
    "    if loss.requires_grad:\n",
    "        loss.backward()\n",
    "        print(\"反向传播成功\")\n",
    "        \n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                print(f\"参数 '{name}' 有梯度\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"没有发现任何参数的梯度\")\n",
    "    else:\n",
    "        print(\"loss没有requires_grad属性\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350bb1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fa8173",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.get_train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd71a718",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_dataset[0]\n",
    "print(sample.keys())\n",
    "print(sample[\"conversation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9b08e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = QwenOmniDataCollator(processor)\n",
    "batch = collator([sample])\n",
    "for k, v in batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = trainer.get_train_dataloader()\n",
    "for b in train_loader:\n",
    "    print(b.keys())\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
