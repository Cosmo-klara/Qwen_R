{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "647ff0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-22 21:55:50,481 - modelscope - INFO - Target directory already exists, skipping creation.\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: ../../Qwen/cache/modelscope/Qwen/Qwen2.5-Omni-3B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98eb44e6dc014202976878987aa5e452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 22,413,312 || all params: 4,734,622,720 || trainable%: 0.4734\n",
      "Model is in training mode: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import torch\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from modelscope import snapshot_download\n",
    "from modelscope import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor, Qwen2_5OmniThinkerForConditionalGeneration\n",
    "import re\n",
    "from transformers.models.qwen2_vl.video_processing_qwen2_vl import Qwen2VLVideoProcessor\n",
    "from qwen_omni_utils import process_mm_info\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from transformers.image_utils import SizeDict\n",
    "\n",
    "def replace_time_tokens_with_percentage(text, time_map, duration):\n",
    "\n",
    "    if not time_map or duration is None:\n",
    "        return text\n",
    "\n",
    "    def repl(match):\n",
    "        token = match.group(0)\n",
    "        if token not in time_map:\n",
    "            return token\n",
    "        t = time_map[token]\n",
    "        pct = t / duration * 100.0\n",
    "        return f\"{pct:.1f}%\"\n",
    "\n",
    "    return re.sub(r\"<s\\d+>|<e\\d+>\", repl, text)\n",
    "\n",
    "\n",
    "class OmniVideoConversationDataset(Dataset):\n",
    "    def __init__(self, json_path: str, video_root: str):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            raw_data = json.load(f)\n",
    "\n",
    "        self.video_root = video_root\n",
    "        self.samples = []\n",
    "\n",
    "        for item in raw_data:\n",
    "            video_id = item[\"id\"]\n",
    "            video_path = os.path.join(video_root, f\"{video_id}.mp4\")\n",
    "            audio_path = video_path.replace(\".mp4\", \".wav\")\n",
    "\n",
    "            convs = item[\"conversations\"]\n",
    "            meta = item.get(\"meta\", {})\n",
    "            duration = meta.get(\"duration\", None)\n",
    "            time_map = meta.get(\"token\", {})\n",
    "            \n",
    "\n",
    "            # 遍历 human / gpt 成对\n",
    "            for i in range(0, len(convs) - 1, 2):\n",
    "                if convs[i][\"from\"] != \"human\" or convs[i + 1][\"from\"] != \"gpt\":\n",
    "                    continue\n",
    "\n",
    "                self.samples.append({\n",
    "                    \"video_path\": video_path,\n",
    "                    \"audio_path\": audio_path,\n",
    "                    \"question\": convs[i][\"value\"],\n",
    "                    \"answer\": convs[i + 1][\"value\"],\n",
    "                    \"duration\": duration,\n",
    "                    \"time_map\": time_map,\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "\n",
    "    def _build_text(self, conversations):\n",
    "        messages = []\n",
    "        for turn in conversations:\n",
    "            if turn[\"from\"] == \"human\":\n",
    "                role = \"user\"\n",
    "            elif turn[\"from\"] == \"gpt\":\n",
    "                role = \"assistant\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            messages.append({\n",
    "                \"role\": role,\n",
    "                \"content\": turn[\"value\"]\n",
    "            })\n",
    "\n",
    "        return messages\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"video\", \"video\": s[\"video_path\"]},\n",
    "                    {\"type\": \"audio\", \"audio\": s[\"audio_path\"]},\n",
    "                    {\"type\": \"text\", \"text\": s[\"question\"]},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": s[\"answer\"]},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"conversation\": conversation,\n",
    "            \"duration\": s[\"duration\"],\n",
    "            \"time_map\": s[\"time_map\"],\n",
    "            }\n",
    "\n",
    "\n",
    "class QwenOmniDataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "        self.tokenizer = processor.tokenizer\n",
    "        self.video_cache = {}\n",
    "\n",
    "    def __call__(self, features):\n",
    "        texts = []\n",
    "        videos = []\n",
    "        audios = []\n",
    "\n",
    "        for f in features:\n",
    "            conversation = f[\"conversation\"]\n",
    "\n",
    "            for msg in conversation:\n",
    "                if msg[\"role\"] in (\"user\", \"assistant\"):\n",
    "                    for ele in msg[\"content\"]:\n",
    "                        if ele.get(\"type\") == \"text\":\n",
    "                            ele[\"text\"] = replace_time_tokens_with_percentage(\n",
    "                                ele[\"text\"],\n",
    "                                f[\"time_map\"],\n",
    "                                f[\"duration\"],\n",
    "                            )\n",
    "\n",
    "            # ---------- 1. 拼完整 prompt ----------\n",
    "            full_text = self.processor.apply_chat_template(\n",
    "                conversation,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False,\n",
    "            )\n",
    "\n",
    "            # ---------- 2. 构造 labels（后缀 assistant） ----------\n",
    "            texts.append(full_text)\n",
    "\n",
    "            # ---------- 3. 多模态 ----------\n",
    "            for msg in conversation:\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    for ele in msg[\"content\"]:\n",
    "                        if ele.get(\"type\") == \"video\":\n",
    "                            ele[\"fps\"] = 0.5\n",
    "                            ele[\"max_frames\"] = 50\n",
    "\n",
    "                            video_path = ele[\"video\"]\n",
    "\n",
    "                            # ---------- 使用缓存 ----------\n",
    "                            if video_path not in self.video_cache:\n",
    "                                audios_, _, videos_ = process_mm_info(\n",
    "                                    conversation, use_audio_in_video=False\n",
    "                                )\n",
    "                                self.video_cache[video_path] = {\n",
    "                                    \"video\": videos_[0] if videos_ else None,\n",
    "                                    \"audio\": audios_[0] if audios_ else None,\n",
    "                                }\n",
    "\n",
    "                            video_tensor = self.video_cache[video_path][\"video\"]\n",
    "                            audio_tensor = self.video_cache[video_path][\"audio\"]\n",
    "\n",
    "\n",
    "            # audios_, _, videos_ = process_mm_info(\n",
    "            #     conversation, use_audio_in_video=True\n",
    "            # )\n",
    "\n",
    "            # videos.append(videos_[0] if videos_ else None)\n",
    "            # audios.append(audios_[0] if audios_ else None)\n",
    "\n",
    "            videos.append(video_tensor)\n",
    "            audios.append(audio_tensor)\n",
    "\n",
    "\n",
    "        # ---------- 4. 一次性 processor ----------\n",
    "        batch = self.processor(\n",
    "            text=texts,\n",
    "            videos=videos,\n",
    "            audio=audios,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            use_audio_in_video=False\n",
    "        )\n",
    "\n",
    "\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        labels[:] = -100\n",
    "\n",
    "        im_start_id = self.tokenizer.convert_tokens_to_ids(\"<|im_start|>\")\n",
    "        assistant_id = self.tokenizer.convert_tokens_to_ids(\"assistant\")\n",
    "        im_end_id = self.tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "\n",
    "        for b in range(labels.size(0)):\n",
    "            input_ids = batch[\"input_ids\"][b]\n",
    "\n",
    "            start = None\n",
    "            for i in range(len(input_ids) - 1):\n",
    "                if input_ids[i] == im_start_id and input_ids[i + 1] == assistant_id:\n",
    "                    start = i + 3\n",
    "                    break\n",
    "\n",
    "            if start is None:\n",
    "                raise RuntimeError(\"No <|im_start|> assistant found\")\n",
    "\n",
    "            end = None\n",
    "            for i in range(start, len(input_ids)):\n",
    "                if input_ids[i] == im_end_id:\n",
    "                    end = i\n",
    "                    break\n",
    "\n",
    "            if end is None:\n",
    "                end = len(input_ids)\n",
    "\n",
    "            labels[b, start:end] = input_ids[start:end]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        print(batch[\"labels\"])\n",
    "\n",
    "        print(batch[\"video_grid_thw\"].shape) \n",
    "\n",
    "        print(batch[\"pixel_values_videos\"].shape) \n",
    "        for k, v in batch.items(): \n",
    "            if isinstance(v, torch.Tensor): \n",
    "                print(k, v.shape, v.numel() * v.element_size() / 1024**3, \"GB\")\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "train_dataset = OmniVideoConversationDataset(\n",
    "    json_path=\"../../LongVALE/data/longvale-sft-bp-7k.json\",\n",
    "    video_root=\"../../LongVALE/raw_videos_train/video_train_7240/\"\n",
    ")\n",
    "\n",
    "\n",
    "model_path = snapshot_download(\n",
    "    'Qwen/Qwen2.5-Omni-3B',\n",
    "    cache_dir=\"../../Qwen/cache/modelscope\"\n",
    ")\n",
    "\n",
    "model = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "class FixedResQwen2VLVideoProcessor(Qwen2VLVideoProcessor):\n",
    "    def _preprocess(\n",
    "        self, videos, do_resize=True, size=None, interpolation=None, **kwargs\n",
    "    ):\n",
    "        # 固定分辨率\n",
    "        fixed_size = SizeDict(height=224, width=224)\n",
    "        for i, video in enumerate(videos):\n",
    "            videos[i] = self.resize(video, size=fixed_size, interpolation=interpolation)\n",
    "        return super()._preprocess(videos, do_resize=False, size=fixed_size, interpolation=interpolation, **kwargs)\n",
    "    \n",
    "video_processor = FixedResQwen2VLVideoProcessor.from_pretrained(model_path)\n",
    "\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(\n",
    "    model_path,\n",
    "    video_processor=video_processor,\n",
    ")\n",
    "\n",
    "\n",
    "# 配置LoRA\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    # task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if (\n",
    "        \"audio_tower\" in name\n",
    "        or \"visual\" in name\n",
    "    ):\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model.config.use_cache = False\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 检查模型是否在训练模式\n",
    "model.train()\n",
    "print(f\"Model is in training mode: {model.training}\")\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./r_models\",\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=2,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=5,\n",
    "    load_best_model_at_end=False,\n",
    ")\n",
    "\n",
    "data_collator = QwenOmniDataCollator(processor)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cde099e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -100,  -100,  -100,  ..., 14360,  -100,  -100]])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([3584, 1176])\n",
      "input_ids torch.Size([1, 2466]) 1.837313175201416e-05 GB\n",
      "attention_mask torch.Size([1, 2466]) 1.837313175201416e-05 GB\n",
      "pixel_values_videos torch.Size([3584, 1176]) 0.0157012939453125 GB\n",
      "video_grid_thw torch.Size([1, 3]) 2.2351741790771484e-08 GB\n",
      "video_second_per_grid torch.Size([1]) 3.725290298461914e-09 GB\n",
      "feature_attention_mask torch.Size([1, 30000]) 0.00011175870895385742 GB\n",
      "input_features torch.Size([1, 128, 30000]) 0.01430511474609375 GB\n",
      "labels torch.Size([1, 2466]) 1.837313175201416e-05 GB\n",
      "tensor([[ -100,  -100,  -100,  ..., 14360,  -100,  -100]])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([6400, 1176])\n",
      "input_ids torch.Size([1, 4866]) 3.625452518463135e-05 GB\n",
      "attention_mask torch.Size([1, 4866]) 3.625452518463135e-05 GB\n",
      "pixel_values_videos torch.Size([6400, 1176]) 0.02803802490234375 GB\n",
      "video_grid_thw torch.Size([1, 3]) 2.2351741790771484e-08 GB\n",
      "video_second_per_grid torch.Size([1]) 3.725290298461914e-09 GB\n",
      "feature_attention_mask torch.Size([1, 30000]) 0.00011175870895385742 GB\n",
      "input_features torch.Size([1, 128, 30000]) 0.01430511474609375 GB\n",
      "labels torch.Size([1, 4866]) 3.625452518463135e-05 GB\n",
      "tensor([[-100, -100, -100,  ...,   13, -100, -100]])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([6400, 1176])\n",
      "input_ids torch.Size([1, 5473]) 4.077702760696411e-05 GB\n",
      "attention_mask torch.Size([1, 5473]) 4.077702760696411e-05 GB\n",
      "pixel_values_videos torch.Size([6400, 1176]) 0.02803802490234375 GB\n",
      "video_grid_thw torch.Size([1, 3]) 2.2351741790771484e-08 GB\n",
      "video_second_per_grid torch.Size([1]) 3.725290298461914e-09 GB\n",
      "feature_attention_mask torch.Size([1, 30000]) 0.00011175870895385742 GB\n",
      "input_features torch.Size([1, 128, 30000]) 0.01430511474609375 GB\n",
      "labels torch.Size([1, 5473]) 4.077702760696411e-05 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='75076' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    3/75076 00:20 < 430:42:54, 0.05 it/s, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0xd39c1e40] mmco: unref short failure\n",
      "[h264 @ 0xd39c1e40] mmco: unref short failure\n",
      "[h264 @ 0xd39c1e40] mmco: unref short failure\n",
      "[h264 @ 0xd39c1e40] mmco: unref short failure\n",
      "[h264 @ 0xd39c1e40] mmco: unref short failure\n",
      "[h264 @ 0xd39c1e40] mmco: unref short failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -100,  -100,  -100,  ..., 14360,  -100,  -100]])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([6400, 1176])\n",
      "input_ids torch.Size([1, 7112]) 5.2988529205322266e-05 GB\n",
      "attention_mask torch.Size([1, 7112]) 5.2988529205322266e-05 GB\n",
      "pixel_values_videos torch.Size([6400, 1176]) 0.02803802490234375 GB\n",
      "video_grid_thw torch.Size([1, 3]) 2.2351741790771484e-08 GB\n",
      "video_second_per_grid torch.Size([1]) 3.725290298461914e-09 GB\n",
      "feature_attention_mask torch.Size([1, 30000]) 0.00011175870895385742 GB\n",
      "input_features torch.Size([1, 128, 30000]) 0.01430511474609375 GB\n",
      "labels torch.Size([1, 7112]) 5.2988529205322266e-05 GB\n",
      "tensor([[-100, -100, -100,  ...,   13, -100, -100]])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([6400, 1176])\n",
      "input_ids torch.Size([1, 7259]) 5.408376455307007e-05 GB\n",
      "attention_mask torch.Size([1, 7259]) 5.408376455307007e-05 GB\n",
      "pixel_values_videos torch.Size([6400, 1176]) 0.02803802490234375 GB\n",
      "video_grid_thw torch.Size([1, 3]) 2.2351741790771484e-08 GB\n",
      "video_second_per_grid torch.Size([1]) 3.725290298461914e-09 GB\n",
      "feature_attention_mask torch.Size([1, 30000]) 0.00011175870895385742 GB\n",
      "input_features torch.Size([1, 128, 30000]) 0.01430511474609375 GB\n",
      "labels torch.Size([1, 7259]) 5.408376455307007e-05 GB\n",
      "tensor([[-100, -100, -100,  ...,   13, -100, -100]])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([6400, 1176])\n",
      "input_ids torch.Size([1, 9152]) 6.818771362304688e-05 GB\n",
      "attention_mask torch.Size([1, 9152]) 6.818771362304688e-05 GB\n",
      "pixel_values_videos torch.Size([6400, 1176]) 0.02803802490234375 GB\n",
      "video_grid_thw torch.Size([1, 3]) 2.2351741790771484e-08 GB\n",
      "video_second_per_grid torch.Size([1]) 3.725290298461914e-09 GB\n",
      "feature_attention_mask torch.Size([1, 30000]) 0.00011175870895385742 GB\n",
      "input_features torch.Size([1, 128, 30000]) 0.01430511474609375 GB\n",
      "labels torch.Size([1, 9152]) 6.818771362304688e-05 GB\n",
      "tensor([[ -100,  -100,  -100,  ..., 14360,  -100,  -100]])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([6400, 1176])\n",
      "input_ids torch.Size([1, 8470]) 6.310641765594482e-05 GB\n",
      "attention_mask torch.Size([1, 8470]) 6.310641765594482e-05 GB\n",
      "pixel_values_videos torch.Size([6400, 1176]) 0.02803802490234375 GB\n",
      "video_grid_thw torch.Size([1, 3]) 2.2351741790771484e-08 GB\n",
      "video_second_per_grid torch.Size([1]) 3.725290298461914e-09 GB\n",
      "feature_attention_mask torch.Size([1, 30000]) 0.00011175870895385742 GB\n",
      "input_features torch.Size([1, 128, 30000]) 0.01430511474609375 GB\n",
      "labels torch.Size([1, 8470]) 6.310641765594482e-05 GB\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.18 GiB. GPU 2 has a total capacity of 23.69 GiB of which 4.10 GiB is free. Process 112003 has 8.25 GiB memory in use. Including non-PyTorch memory, this process has 11.31 GiB memory in use. Of the allocated memory 10.96 GiB is allocated by PyTorch, and 41.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2672\u001b[0m )\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2680\u001b[0m ):\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/trainer.py:4071\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   4069\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 4071\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/accelerate/accelerator.py:2852\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2851\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2852\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 2 has a total capacity of 23.69 GiB of which 4.10 GiB is free. Process 112003 has 8.25 GiB memory in use. Including non-PyTorch memory, this process has 11.31 GiB memory in use. Of the allocated memory 10.96 GiB is allocated by PyTorch, and 41.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import signature\n",
    "print(signature(Qwen2_5OmniForConditionalGeneration.forward))\n",
    "print(signature(Qwen2_5OmniThinkerForConditionalGeneration.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67b27ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_lora_gradients(model):\n",
    "    \"\"\"验证LoRA梯度流\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # 创建测试数据\n",
    "    test_inputs = {\n",
    "        'input_ids': torch.randint(0, 1000, (1, 10)).cuda(),\n",
    "        'attention_mask': torch.ones(1, 10).cuda(),\n",
    "        'labels': torch.randint(0, 1000, (1, 10)).cuda(),\n",
    "    }\n",
    "    \n",
    "    # 前向传播\n",
    "    outputs = model(**test_inputs)\n",
    "    loss = outputs.loss\n",
    "    \n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "    print(f\"Loss requires_grad: {loss.requires_grad}\")\n",
    "    \n",
    "    if loss.requires_grad:\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 检查梯度\n",
    "        gradients_found = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                gradients_found = True\n",
    "                grad_norm = param.grad.norm().item()\n",
    "                if 'lora' in name:\n",
    "                    print(f\"  ✓ LoRA梯度: {name} | norm={grad_norm:.6f}\")\n",
    "        \n",
    "        if not gradients_found:\n",
    "            print(\"  ⚠ 没有找到梯度\")\n",
    "    else:\n",
    "        print(\"  ✗ Loss没有requires_grad\")\n",
    "\n",
    "verify_lora_gradients(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c6c712",
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m sample \u001b[38;5;241m=\u001b[39m train_dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 通过 data_collator 生成 batch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]       \u001b[38;5;66;03m# [batch_size, seq_len]\u001b[39;00m\n\u001b[1;32m      8\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m# [batch_size, seq_len]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 188\u001b[0m, in \u001b[0;36mQwenOmniDataCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    184\u001b[0m     audios\u001b[38;5;241m.\u001b[39mappend(audio_tensor)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ---------- 4. 一次性 processor ----------\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_audio_in_video\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    195\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    199\u001b[0m labels[:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py:201\u001b[0m, in \u001b[0;36mQwen2_5OmniProcessor.__call__\u001b[0;34m(self, text, images, videos, audio, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     text \u001b[38;5;241m=\u001b[39m [text]\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m videos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m audio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace_multimodal_special_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43maudio_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideo_second_per_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_second_per_grid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_audio_in_video\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_audio_in_video\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_id_per_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_id_per_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseconds_per_chunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseconds_per_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m texts_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutput_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchFeature(\n\u001b[1;32m    215\u001b[0m     data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtexts_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mimages_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvideos_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maudio_inputs},\n\u001b[1;32m    216\u001b[0m     tensor_type\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    217\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py:244\u001b[0m, in \u001b[0;36mQwen2_5OmniProcessor.replace_multimodal_special_tokens\u001b[0;34m(self, text, audio_lengths, image_grid_thw, video_grid_thw, video_second_per_grid, use_audio_in_video, position_id_per_seconds, seconds_per_chunk)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, special_token \u001b[38;5;129;01min\u001b[39;00m positions:\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m special_token \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_token:\n\u001b[0;32m--> 244\u001b[0m         sample \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_token, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|audio_placeholder|>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maudio_lengths\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m special_token \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_token:\n\u001b[1;32m    246\u001b[0m         image_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(image_grid_thw)\u001b[38;5;241m.\u001b[39mprod() \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m merge_length_image\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 取训练集第一个样本\n",
    "sample = train_dataset[0]\n",
    "\n",
    "# 通过 data_collator 生成 batch\n",
    "batch = data_collator([sample])\n",
    "\n",
    "labels = batch[\"labels\"]       # [batch_size, seq_len]\n",
    "input_ids = batch[\"input_ids\"] # [batch_size, seq_len]\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "for i in range(labels.shape[0]):\n",
    "    effective_ids = []\n",
    "    ignored_ids = []\n",
    "    \n",
    "    for j in range(labels.shape[1]):\n",
    "        token_id = input_ids[i, j].item()\n",
    "        label = labels[i, j].item()\n",
    "        if label != -100:\n",
    "            effective_ids.append(token_id)\n",
    "        else:\n",
    "            ignored_ids.append(token_id)\n",
    "    \n",
    "    effective_text = tokenizer.decode(effective_ids)\n",
    "    ignored_text = tokenizer.decode(ignored_ids)\n",
    "    \n",
    "    print(f\"=== Sample {i} ===\")\n",
    "    print(\"有效 token 拼接文本:\")\n",
    "    print(effective_text)\n",
    "    print(\"无效 token 拼接文本:\")\n",
    "    print(ignored_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2834ad",
   "metadata": {},
   "source": [
    "=== Sample 0 ===\n",
    "有效 token 拼接文本:\n",
    "From <s4> to <e4>.\n",
    "\n",
    "这里有个问题，<s4> 转化成什么时间\n",
    "- 用真实时间？\n",
    "    因为 Qwen 实际上能学习到真实的时间，但是评估的时候要算成相对时间吗？还是直接和数据集中的计算？但是这样和 LongVale 算是不是不太公平\n",
    "- 和 LongVale 一样用相对时间百分比\n",
    "    感觉这个是对应的固定 100 帧，如果 Qwen 也固定一百帧的话（不太确定这样能不能对应成绝对时间？），而且固定 100帧 目前显存也不够\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f9df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== 进行前向传播测试 ===\")\n",
    "with torch.set_grad_enabled(True):\n",
    "\n",
    "    sample = next(iter(train_dataset))\n",
    "    batch = data_collator([sample])\n",
    "    \n",
    "\n",
    "    device = model.device\n",
    "    batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "    \n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    \n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Loss requires_grad: {loss.requires_grad}\")\n",
    "    \n",
    "    if loss.requires_grad:\n",
    "        loss.backward()\n",
    "        print(\"反向传播成功\")\n",
    "        \n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                print(f\"参数 '{name}' 有梯度\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"没有发现任何参数的梯度\")\n",
    "    else:\n",
    "        print(\"loss没有requires_grad属性\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "350bb1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75075\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57fa8173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f0a304428f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.get_train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd71a718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['conversation', 'duration', 'time_map'])\n",
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.'}]}, {'role': 'user', 'content': [{'type': 'video', 'video': '../../LongVALE/raw_videos_train/video_train_7240/cNj_TMPKa10.mp4'}, {'type': 'audio', 'audio': '../../LongVALE/raw_videos_train/video_train_7240/cNj_TMPKa10.wav'}, {'type': 'text', 'text': '<video>\\nDuring which frames in the video can we observe someone lifts a freshly prepared taco, adorned with cilantro and a dollop of white sauce, from a wooden platter, a juicy lime wedge resting beside it, as a man excitedly remarks on an \"East connection\" amidst the lively chatter happening?'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': 'From <s4> to <e4>.'}]}]\n"
     ]
    }
   ],
   "source": [
    "sample = train_dataset[0]\n",
    "print(sample.keys())\n",
    "print(sample[\"conversation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd9b08e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m collator \u001b[38;5;241m=\u001b[39m QwenOmniDataCollator(processor)\n\u001b[0;32m----> 2\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43mcollator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "Cell \u001b[0;32mIn[1], line 189\u001b[0m, in \u001b[0;36mQwenOmniDataCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    185\u001b[0m     audios\u001b[38;5;241m.\u001b[39mappend(audio_tensor)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# ---------- 4. 一次性 processor ----------\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_audio_in_video\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    200\u001b[0m labels[:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py:201\u001b[0m, in \u001b[0;36mQwen2_5OmniProcessor.__call__\u001b[0;34m(self, text, images, videos, audio, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     text \u001b[38;5;241m=\u001b[39m [text]\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m videos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m audio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace_multimodal_special_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43maudio_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideo_second_per_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_second_per_grid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_audio_in_video\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_audio_in_video\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_id_per_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_id_per_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseconds_per_chunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseconds_per_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m texts_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutput_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchFeature(\n\u001b[1;32m    215\u001b[0m     data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtexts_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mimages_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvideos_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maudio_inputs},\n\u001b[1;32m    216\u001b[0m     tensor_type\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    217\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py:244\u001b[0m, in \u001b[0;36mQwen2_5OmniProcessor.replace_multimodal_special_tokens\u001b[0;34m(self, text, audio_lengths, image_grid_thw, video_grid_thw, video_second_per_grid, use_audio_in_video, position_id_per_seconds, seconds_per_chunk)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, special_token \u001b[38;5;129;01min\u001b[39;00m positions:\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m special_token \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_token:\n\u001b[0;32m--> 244\u001b[0m         sample \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_token, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|audio_placeholder|>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maudio_lengths\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m special_token \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_token:\n\u001b[1;32m    246\u001b[0m         image_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(image_grid_thw)\u001b[38;5;241m.\u001b[39mprod() \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m merge_length_image\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "collator = QwenOmniDataCollator(processor)\n",
    "batch = collator([sample])\n",
    "for k, v in batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9eb639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = trainer.get_train_dataloader()\n",
    "for b in train_loader:\n",
    "    print(b.keys())\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
