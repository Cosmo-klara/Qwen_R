{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d9fd77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,4,5,6\"\n",
    "import torch\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from modelscope import snapshot_download\n",
    "from modelscope import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor, Qwen2_5OmniThinkerForConditionalGeneration\n",
    "\n",
    "from transformers.models.qwen2_vl.video_processing_qwen2_vl import Qwen2VLVideoProcessor\n",
    "from transformers.video_utils import VideoMetadata\n",
    "from typing import Optional\n",
    "from qwen_omni_utils import process_mm_info\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from transformers.image_utils import SizeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b20277ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedResQwen2VLVideoProcessor(Qwen2VLVideoProcessor):\n",
    "    def _preprocess(\n",
    "        self, videos, do_resize=True, size=None, interpolation=None, **kwargs\n",
    "    ):\n",
    "        # 固定分辨率\n",
    "        fixed_size = SizeDict(height=224, width=224)\n",
    "        for i, video in enumerate(videos):\n",
    "            videos[i] = self.resize(video, size=fixed_size, interpolation=interpolation)\n",
    "        return super()._preprocess(videos, do_resize=False, size=fixed_size, interpolation=interpolation, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48cc2787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniVideoConversationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        json_path: str,\n",
    "        video_root: str,\n",
    "    ):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "        self.video_root = video_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _build_text(self, conversations):\n",
    "        messages = []\n",
    "        for turn in conversations:\n",
    "            if turn[\"from\"] == \"human\":\n",
    "                role = \"user\"\n",
    "            elif turn[\"from\"] == \"gpt\":\n",
    "                role = \"assistant\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            messages.append({\n",
    "                \"role\": role,\n",
    "                \"content\": turn[\"value\"]\n",
    "            })\n",
    "\n",
    "        return messages\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        video_id = sample[\"id\"]\n",
    "        video_path = os.path.join(self.video_root, f\"{video_id}.mp4\")\n",
    "\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team.\"}\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"video\", \"video\": video_path},\n",
    "                    {\"type\": \"text\", \"text\": sample[\"conversations\"][0][\"value\"]},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": sample[\"conversations\"][1][\"value\"]},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        return {\"conversation\": conversation}\n",
    "\n",
    "\n",
    "\n",
    "class QwenOmniDataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, features):\n",
    "        texts = []\n",
    "        all_videos = []\n",
    "        all_audios = []\n",
    "\n",
    "        for f in features:\n",
    "            conversation = f[\"conversation\"]\n",
    "\n",
    "            text = self.processor.apply_chat_template(\n",
    "                conversation,\n",
    "                add_generation_prompt=False,\n",
    "                tokenize=False,\n",
    "            )\n",
    "            for msg in conversation:\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    for ele in msg[\"content\"]:\n",
    "                        if ele.get(\"type\") == \"video\":\n",
    "                            ele[\"fps\"] = 0.5   # 设置 1fps\n",
    "\n",
    "            audios, images, videos = process_mm_info(\n",
    "                conversation, use_audio_in_video=True\n",
    "            )\n",
    "\n",
    "            texts.append(text)\n",
    "            all_videos.append(videos[0] if videos else None)\n",
    "            all_audios.append(audios[0] if audios else None)\n",
    "\n",
    "        batch = self.processor(\n",
    "            text=texts,\n",
    "            videos=all_videos,\n",
    "            audio=all_audios,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            use_audio_in_video=True,\n",
    "        )\n",
    "\n",
    "        print(batch[\"video_grid_thw\"].shape) \n",
    "        print(batch[\"pixel_values_videos\"].shape) \n",
    "        for k, v in batch.items(): \n",
    "            if isinstance(v, torch.Tensor): \n",
    "                print(k, v.shape, v.numel() * v.element_size() / 1024**3, \"GB\")\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "train_dataset = OmniVideoConversationDataset(\n",
    "    json_path=\"../../LongVALE/data/longvale-sft-bp-7k.json\",\n",
    "    video_root=\"../../LongVALE/raw_videos_train/video_train_7240/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74658acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: ../../Qwen/cache/modelscope/Qwen/Qwen2.5-Omni-3B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 13:47:47,417 - modelscope - INFO - Target directory already exists, skipping creation.\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995c742d8d554c8fa8cb29d28ccef73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    }
   ],
   "source": [
    "model_path = snapshot_download(\n",
    "    'Qwen/Qwen2.5-Omni-3B',\n",
    "    cache_dir=\"../../Qwen/cache/modelscope\"\n",
    ")\n",
    "\n",
    "model = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"balanced\",\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "video_processor = FixedResQwen2VLVideoProcessor.from_pretrained(model_path)\n",
    "\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(\n",
    "    model_path,\n",
    "    video_processor=video_processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c03a6e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 22,413,312 || all params: 4,734,622,720 || trainable%: 0.4734\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"up_proj\", \"down_proj\"],\n",
    "    # target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if (\n",
    "        \"audio_tower\" in name\n",
    "        or \"visual\" in name\n",
    "    ):\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b419779",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./r_models\",\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=2,\n",
    "    # per_device_eval_batch_size=batch_size,\n",
    "    bf16=True,\n",
    "    num_train_epochs=2, # 5 -> 2\n",
    "    logging_steps=5,\n",
    "    load_best_model_at_end=False,\n",
    "    label_names=[\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8f754d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "data_collator = QwenOmniDataCollator(processor)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    # model=model.thinker,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a06c894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.FixedResQwen2VLVideoProcessor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(processor.video_processor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f1a73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:System prompt modified, audio output may not work as expected. Audio output mode only works when using default system prompt 'You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.'\n",
      "/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/qwen_omni_utils/v2_5/audio_process.py:85: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  librosa.load(\n",
      "/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
