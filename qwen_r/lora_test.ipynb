{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef80fea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'\n",
    "# os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76c9691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from modelscope import snapshot_download\n",
    "from modelscope import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor, Qwen2_5OmniThinkerForConditionalGeneration\n",
    "\n",
    "from transformers.models.qwen2_vl.video_processing_qwen2_vl import Qwen2VLVideoProcessor\n",
    "from transformers.video_utils import VideoMetadata\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb616b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class OmniVideoConversationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        json_path: str,\n",
    "        video_root: str,\n",
    "    ):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "        self.video_root = video_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _build_text(self, conversations):\n",
    "        messages = []\n",
    "        for turn in conversations:\n",
    "            if turn[\"from\"] == \"human\":\n",
    "                role = \"user\"\n",
    "            elif turn[\"from\"] == \"gpt\":\n",
    "                role = \"assistant\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            messages.append({\n",
    "                \"role\": role,\n",
    "                \"content\": turn[\"value\"]\n",
    "            })\n",
    "\n",
    "        return messages\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        messages = self._build_text(sample[\"conversations\"])\n",
    "        video_id = sample[\"id\"]\n",
    "        video_path = os.path.join(self.video_root, f\"{video_id}.mp4\")\n",
    "\n",
    "        return {\n",
    "            \"text\": messages,\n",
    "            \"videos\": [video_path],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09feda9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = OmniVideoConversationDataset(\n",
    "    json_path=\"../../LongVALE/data/longvale-sft-bp-7k.json\",\n",
    "    video_root=\"../../LongVALE/raw_videos_train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac428f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedFrameQwen2VLVideoProcessor(Qwen2VLVideoProcessor):\n",
    "    def __init__(self, num_frames=100, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fixed_num_frames = num_frames\n",
    "\n",
    "    def sample_frames(\n",
    "        self,\n",
    "        metadata: VideoMetadata,\n",
    "        temporal_patch_size: Optional[int] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        temporal_patch_size = temporal_patch_size or self.temporal_patch_size\n",
    "\n",
    "        # 对齐 temporal_patch_size（Qwen2.5 默认 = 2）\n",
    "        num_frames = round(self.fixed_num_frames / temporal_patch_size) * temporal_patch_size\n",
    "        num_frames = min(num_frames, metadata.total_num_frames)\n",
    "\n",
    "        indices = torch.linspace(\n",
    "            0,\n",
    "            metadata.total_num_frames - 1,\n",
    "            steps=num_frames,\n",
    "        ).long()\n",
    "\n",
    "        return indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5663cb",
   "metadata": {},
   "source": [
    "thinker\n",
    "\n",
    "github上看到还有就是用 model= model.thinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9235377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: ../../Qwen/cache/modelscope/Qwen/Qwen2.5-Omni-3B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 00:23:59,144 - modelscope - INFO - Target directory already exists, skipping creation.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f148e9af9f744073bebb475d3c0af179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    }
   ],
   "source": [
    "model_path = snapshot_download(\n",
    "    'Qwen/Qwen2.5-Omni-3B',\n",
    "    cache_dir=\"../../Qwen/cache/modelscope\"\n",
    ")\n",
    "\n",
    "model = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "# model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n",
    "#     model_path,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"cuda:1\",\n",
    "#     trust_remote_code=True,\n",
    "#     enable_audio_output=False,\n",
    "#     use_safetensors=True\n",
    "# )\n",
    "\n",
    "# processor = Qwen2_5OmniProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "video_processor = FixedFrameQwen2VLVideoProcessor.from_pretrained(model_path)\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(\n",
    "    model_path,\n",
    "    video_processor=video_processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3a4684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = snapshot_download(\n",
    "#     'Qwen/Qwen2.5-Omni-3B',\n",
    "#     cache_dir=\"./cache/modelscope\"\n",
    "# )\n",
    "\n",
    "# model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n",
    "#     model_path,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"cuda:1\",\n",
    "#     trust_remote_code=True,\n",
    "#     enable_audio_output=False,\n",
    "#     use_safetensors=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78e96235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = Qwen2_5OmniThinkerForConditionalGeneration(\n",
      "  (audio_tower): Qwen2_5OmniAudioEncoder(\n",
      "    (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (positional_embedding): SinusoidsPositionEmbedding()\n",
      "    (audio_bos_eos_token): Embedding(2, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x Qwen2_5OmniAudioEncoderLayer(\n",
      "        (self_attn): Qwen2_5OmniAudioAttention(\n",
      "          (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): GELUActivation()\n",
      "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    (avg_pooler): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "    (proj): Linear(in_features=1280, out_features=2048, bias=True)\n",
      "  )\n",
      "  (visual): Qwen2_5OmniVisionEncoder(\n",
      "    (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
      "      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
      "    )\n",
      "    (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
      "    (blocks): ModuleList(\n",
      "      (0-31): 32 x Qwen2_5OmniVisionBlock(\n",
      "        (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "        (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "        (attn): Qwen2_5OmniVisionAttention(\n",
      "          (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        )\n",
      "        (mlp): Qwen2_5OmniMLP(\n",
      "          (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "          (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "          (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (merger): Qwen2_5OmniPatchMerger(\n",
      "      (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=5120, out_features=2048, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (model): Qwen2_5OmniThinkerTextModel(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-35): 36 x Qwen2_5OmniDecoderLayer(\n",
      "        (self_attn): Qwen2_5OmniAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      ")\n",
      "audio_tower = Qwen2_5OmniAudioEncoder(\n",
      "  (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (positional_embedding): SinusoidsPositionEmbedding()\n",
      "  (audio_bos_eos_token): Embedding(2, 2048)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x Qwen2_5OmniAudioEncoderLayer(\n",
      "      (self_attn): Qwen2_5OmniAudioAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (activation_fn): GELUActivation()\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (avg_pooler): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  (proj): Linear(in_features=1280, out_features=2048, bias=True)\n",
      ")\n",
      "audio_tower.conv1 = Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "audio_tower.conv2 = Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "audio_tower.positional_embedding = SinusoidsPositionEmbedding()\n",
      "audio_tower.audio_bos_eos_token = Embedding(2, 2048)\n",
      "audio_tower.layers = ModuleList(\n",
      "  (0-31): 32 x Qwen2_5OmniAudioEncoderLayer(\n",
      "    (self_attn): Qwen2_5OmniAudioAttention(\n",
      "      (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "      (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    )\n",
      "    (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    (activation_fn): GELUActivation()\n",
      "    (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "    (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "    (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "audio_tower.layers.0 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.0.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.0.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.0.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.0.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.0.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.0.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.0.activation_fn = GELUActivation()\n",
      "audio_tower.layers.0.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.0.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.0.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.1 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.1.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.1.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.1.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.1.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.1.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.1.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.1.activation_fn = GELUActivation()\n",
      "audio_tower.layers.1.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.1.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.1.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.2 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.2.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.2.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.2.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.2.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.2.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.2.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.2.activation_fn = GELUActivation()\n",
      "audio_tower.layers.2.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.2.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.2.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.3 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.3.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.3.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.3.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.3.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.3.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.3.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.3.activation_fn = GELUActivation()\n",
      "audio_tower.layers.3.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.3.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.3.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.4 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.4.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.4.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.4.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.4.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.4.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.4.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.4.activation_fn = GELUActivation()\n",
      "audio_tower.layers.4.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.4.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.4.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.5 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.5.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.5.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.5.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.5.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.5.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.5.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.5.activation_fn = GELUActivation()\n",
      "audio_tower.layers.5.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.5.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.5.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.6 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.6.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.6.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.6.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.6.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.6.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.6.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.6.activation_fn = GELUActivation()\n",
      "audio_tower.layers.6.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.6.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.6.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.7 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.7.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.7.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.7.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.7.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.7.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.7.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.7.activation_fn = GELUActivation()\n",
      "audio_tower.layers.7.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.7.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.7.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.8 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.8.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.8.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.8.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.8.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.8.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.8.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.8.activation_fn = GELUActivation()\n",
      "audio_tower.layers.8.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.8.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.8.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.9 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.9.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.9.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.9.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.9.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.9.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.9.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.9.activation_fn = GELUActivation()\n",
      "audio_tower.layers.9.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.9.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.9.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.10 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.10.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.10.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.10.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.10.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.10.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.10.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.10.activation_fn = GELUActivation()\n",
      "audio_tower.layers.10.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.10.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.10.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.11 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.11.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.11.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.11.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.11.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.11.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.11.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.11.activation_fn = GELUActivation()\n",
      "audio_tower.layers.11.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.11.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.11.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.12 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.12.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.12.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.12.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.12.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.12.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.12.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.12.activation_fn = GELUActivation()\n",
      "audio_tower.layers.12.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.12.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.12.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.13 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.13.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.13.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.13.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.13.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.13.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.13.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.13.activation_fn = GELUActivation()\n",
      "audio_tower.layers.13.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.13.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.13.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.14 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.14.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.14.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.14.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.14.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.14.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.14.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.14.activation_fn = GELUActivation()\n",
      "audio_tower.layers.14.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.14.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.14.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.15 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.15.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.15.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.15.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.15.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.15.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.15.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.15.activation_fn = GELUActivation()\n",
      "audio_tower.layers.15.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.15.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.15.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.16 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.16.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.16.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.16.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.16.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.16.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.16.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.16.activation_fn = GELUActivation()\n",
      "audio_tower.layers.16.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.16.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.16.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.17 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.17.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.17.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.17.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.17.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.17.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.17.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.17.activation_fn = GELUActivation()\n",
      "audio_tower.layers.17.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.17.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.17.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.18 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.18.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.18.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.18.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.18.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.18.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.18.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.18.activation_fn = GELUActivation()\n",
      "audio_tower.layers.18.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.18.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.18.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.19 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.19.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.19.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.19.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.19.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.19.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.19.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.19.activation_fn = GELUActivation()\n",
      "audio_tower.layers.19.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.19.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.19.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.20 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.20.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.20.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.20.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.20.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.20.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.20.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.20.activation_fn = GELUActivation()\n",
      "audio_tower.layers.20.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.20.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.20.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.21 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.21.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.21.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.21.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.21.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.21.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.21.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.21.activation_fn = GELUActivation()\n",
      "audio_tower.layers.21.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.21.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.21.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.22 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.22.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.22.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.22.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.22.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.22.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.22.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.22.activation_fn = GELUActivation()\n",
      "audio_tower.layers.22.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.22.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.22.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.23 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.23.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.23.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.23.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.23.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.23.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.23.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.23.activation_fn = GELUActivation()\n",
      "audio_tower.layers.23.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.23.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.23.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.24 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.24.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.24.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.24.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.24.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.24.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.24.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.24.activation_fn = GELUActivation()\n",
      "audio_tower.layers.24.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.24.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.24.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.25 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.25.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.25.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.25.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.25.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.25.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.25.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.25.activation_fn = GELUActivation()\n",
      "audio_tower.layers.25.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.25.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.25.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.26 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.26.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.26.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.26.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.26.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.26.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.26.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.26.activation_fn = GELUActivation()\n",
      "audio_tower.layers.26.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.26.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.26.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.27 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.27.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.27.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.27.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.27.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.27.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.27.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.27.activation_fn = GELUActivation()\n",
      "audio_tower.layers.27.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.27.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.27.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.28 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.28.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.28.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.28.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.28.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.28.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.28.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.28.activation_fn = GELUActivation()\n",
      "audio_tower.layers.28.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.28.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.28.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.29 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.29.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.29.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.29.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.29.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.29.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.29.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.29.activation_fn = GELUActivation()\n",
      "audio_tower.layers.29.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.29.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.29.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.30 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.30.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.30.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.30.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.30.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.30.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.30.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.30.activation_fn = GELUActivation()\n",
      "audio_tower.layers.30.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.30.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.30.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.31 = Qwen2_5OmniAudioEncoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAudioAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "audio_tower.layers.31.self_attn = Qwen2_5OmniAudioAttention(\n",
      "  (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "  (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "audio_tower.layers.31.self_attn.k_proj = Linear(in_features=1280, out_features=1280, bias=False)\n",
      "audio_tower.layers.31.self_attn.v_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.31.self_attn.q_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.31.self_attn.out_proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "audio_tower.layers.31.self_attn_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.layers.31.activation_fn = GELUActivation()\n",
      "audio_tower.layers.31.fc1 = Linear(in_features=1280, out_features=5120, bias=True)\n",
      "audio_tower.layers.31.fc2 = Linear(in_features=5120, out_features=1280, bias=True)\n",
      "audio_tower.layers.31.final_layer_norm = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.ln_post = LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "audio_tower.avg_pooler = AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "audio_tower.proj = Linear(in_features=1280, out_features=2048, bias=True)\n",
      "visual = Qwen2_5OmniVisionEncoder(\n",
      "  (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
      "    (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
      "  )\n",
      "  (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
      "  (blocks): ModuleList(\n",
      "    (0-31): 32 x Qwen2_5OmniVisionBlock(\n",
      "      (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "      (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "      (attn): Qwen2_5OmniVisionAttention(\n",
      "        (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (mlp): Qwen2_5OmniMLP(\n",
      "        (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "        (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "        (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (merger): Qwen2_5OmniPatchMerger(\n",
      "    (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=5120, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "visual.patch_embed = Qwen2_5_VisionPatchEmbed(\n",
      "  (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
      ")\n",
      "visual.patch_embed.proj = Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
      "visual.rotary_pos_emb = Qwen2_5_VisionRotaryEmbedding()\n",
      "visual.blocks = ModuleList(\n",
      "  (0-31): 32 x Qwen2_5OmniVisionBlock(\n",
      "    (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "    (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "    (attn): Qwen2_5OmniVisionAttention(\n",
      "      (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    )\n",
      "    (mlp): Qwen2_5OmniMLP(\n",
      "      (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "      (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "      (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "      (act_fn): SiLUActivation()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "visual.blocks.0 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.0.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.0.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.0.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.0.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.0.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.0.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.0.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.0.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.0.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.0.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.0.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.0.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.1 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.1.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.1.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.1.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.1.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.1.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.1.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.1.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.1.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.1.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.1.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.1.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.1.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.2 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.2.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.2.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.2.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.2.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.2.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.2.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.2.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.2.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.2.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.2.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.2.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.2.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.3 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.3.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.3.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.3.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.3.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.3.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.3.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.3.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.3.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.3.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.3.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.3.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.3.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.4 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.4.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.4.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.4.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.4.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.4.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.4.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.4.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.4.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.4.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.4.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.4.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.4.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.5 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.5.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.5.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.5.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.5.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.5.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.5.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.5.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.5.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.5.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.5.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.5.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.5.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.6 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.6.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.6.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.6.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.6.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.6.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.6.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.6.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.6.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.6.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.6.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.6.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.6.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.7 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.7.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.7.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.7.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.7.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.7.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.7.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.7.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.7.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.7.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.7.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.7.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.7.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.8 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.8.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.8.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.8.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.8.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.8.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.8.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.8.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.8.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.8.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.8.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.8.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.8.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.9 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.9.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.9.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.9.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.9.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.9.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.9.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.9.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.9.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.9.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.9.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.9.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.9.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.10 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.10.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.10.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.10.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.10.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.10.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.10.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.10.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.10.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.10.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.10.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.10.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.10.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.11 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.11.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.11.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.11.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.11.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.11.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.11.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.11.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.11.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.11.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.11.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.11.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.11.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.12 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.12.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.12.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.12.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.12.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.12.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.12.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.12.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.12.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.12.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.12.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.12.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.12.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.13 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.13.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.13.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.13.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.13.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.13.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.13.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.13.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.13.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.13.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.13.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.13.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.13.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.14 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.14.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.14.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.14.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.14.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.14.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.14.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.14.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.14.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.14.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.14.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.14.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.14.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.15 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.15.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.15.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.15.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.15.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.15.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.15.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.15.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.15.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.15.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.15.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.15.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.15.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.16 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.16.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.16.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.16.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.16.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.16.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.16.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.16.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.16.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.16.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.16.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.16.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.16.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.17 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.17.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.17.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.17.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.17.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.17.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.17.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.17.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.17.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.17.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.17.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.17.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.17.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.18 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.18.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.18.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.18.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.18.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.18.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.18.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.18.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.18.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.18.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.18.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.18.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.18.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.19 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.19.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.19.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.19.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.19.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.19.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.19.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.19.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.19.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.19.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.19.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.19.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.19.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.20 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.20.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.20.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.20.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.20.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.20.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.20.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.20.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.20.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.20.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.20.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.20.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.20.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.21 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.21.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.21.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.21.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.21.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.21.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.21.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.21.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.21.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.21.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.21.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.21.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.21.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.22 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.22.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.22.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.22.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.22.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.22.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.22.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.22.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.22.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.22.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.22.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.22.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.22.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.23 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.23.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.23.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.23.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.23.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.23.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.23.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.23.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.23.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.23.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.23.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.23.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.23.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.24 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.24.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.24.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.24.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.24.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.24.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.24.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.24.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.24.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.24.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.24.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.24.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.24.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.25 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.25.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.25.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.25.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.25.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.25.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.25.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.25.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.25.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.25.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.25.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.25.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.25.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.26 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.26.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.26.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.26.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.26.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.26.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.26.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.26.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.26.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.26.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.26.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.26.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.26.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.27 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.27.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.27.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.27.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.27.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.27.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.27.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.27.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.27.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.27.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.27.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.27.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.27.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.28 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.28.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.28.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.28.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.28.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.28.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.28.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.28.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.28.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.28.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.28.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.28.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.28.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.29 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.29.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.29.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.29.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.29.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.29.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.29.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.29.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.29.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.29.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.29.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.29.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.29.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.30 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.30.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.30.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.30.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.30.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.30.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.30.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.30.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.30.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.30.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.30.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.30.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.30.mlp.act_fn = SiLUActivation()\n",
      "visual.blocks.31 = Qwen2_5OmniVisionBlock(\n",
      "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (attn): Qwen2_5OmniVisionAttention(\n",
      "    (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (mlp): Qwen2_5OmniMLP(\n",
      "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "visual.blocks.31.norm1 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.31.norm2 = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.blocks.31.attn = Qwen2_5OmniVisionAttention(\n",
      "  (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      ")\n",
      "visual.blocks.31.attn.q = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.31.attn.k = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.31.attn.v = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.31.attn.proj = Linear(in_features=1280, out_features=1280, bias=True)\n",
      "visual.blocks.31.mlp = Qwen2_5OmniMLP(\n",
      "  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "visual.blocks.31.mlp.gate_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.31.mlp.up_proj = Linear(in_features=1280, out_features=3420, bias=True)\n",
      "visual.blocks.31.mlp.down_proj = Linear(in_features=3420, out_features=1280, bias=True)\n",
      "visual.blocks.31.mlp.act_fn = SiLUActivation()\n",
      "visual.merger = Qwen2_5OmniPatchMerger(\n",
      "  (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=5120, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "visual.merger.ln_q = Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "visual.merger.mlp = Sequential(\n",
      "  (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=5120, out_features=2048, bias=True)\n",
      ")\n",
      "visual.merger.mlp.0 = Linear(in_features=5120, out_features=5120, bias=True)\n",
      "visual.merger.mlp.1 = GELU(approximate='none')\n",
      "visual.merger.mlp.2 = Linear(in_features=5120, out_features=2048, bias=True)\n",
      "model = Qwen2_5OmniThinkerTextModel(\n",
      "  (embed_tokens): Embedding(151936, 2048)\n",
      "  (layers): ModuleList(\n",
      "    (0-35): 36 x Qwen2_5OmniDecoderLayer(\n",
      "      (self_attn): Qwen2_5OmniAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): Qwen2MLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "        (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "      (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.embed_tokens = Embedding(151936, 2048)\n",
      "model.layers = ModuleList(\n",
      "  (0-35): 36 x Qwen2_5OmniDecoderLayer(\n",
      "    (self_attn): Qwen2_5OmniAttention(\n",
      "      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "      (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "      (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "      (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): Qwen2MLP(\n",
      "      (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "      (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "      (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "      (act_fn): SiLUActivation()\n",
      "    )\n",
      "    (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "    (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  )\n",
      ")\n",
      "model.layers.0 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.0.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.0.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.0.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.0.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.0.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.0.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.0.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.0.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.0.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.0.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.0.mlp.act_fn = SiLUActivation()\n",
      "model.layers.0.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.0.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.1 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.1.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.1.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.1.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.1.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.1.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.1.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.1.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.1.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.1.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.1.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.1.mlp.act_fn = SiLUActivation()\n",
      "model.layers.1.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.1.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.2 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.2.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.2.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.2.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.2.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.2.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.2.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.2.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.2.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.2.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.2.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.2.mlp.act_fn = SiLUActivation()\n",
      "model.layers.2.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.2.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.3 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.3.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.3.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.3.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.3.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.3.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.3.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.3.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.3.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.3.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.3.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.3.mlp.act_fn = SiLUActivation()\n",
      "model.layers.3.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.3.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.4 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.4.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.4.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.4.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.4.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.4.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.4.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.4.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.4.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.4.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.4.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.4.mlp.act_fn = SiLUActivation()\n",
      "model.layers.4.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.4.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.5 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.5.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.5.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.5.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.5.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.5.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.5.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.5.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.5.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.5.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.5.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.5.mlp.act_fn = SiLUActivation()\n",
      "model.layers.5.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.5.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.6 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.6.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.6.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.6.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.6.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.6.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.6.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.6.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.6.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.6.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.6.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.6.mlp.act_fn = SiLUActivation()\n",
      "model.layers.6.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.6.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.7 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.7.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.7.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.7.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.7.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.7.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.7.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.7.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.7.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.7.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.7.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.7.mlp.act_fn = SiLUActivation()\n",
      "model.layers.7.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.7.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.8 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.8.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.8.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.8.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.8.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.8.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.8.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.8.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.8.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.8.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.8.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.8.mlp.act_fn = SiLUActivation()\n",
      "model.layers.8.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.8.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.9 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.9.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.9.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.9.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.9.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.9.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.9.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.9.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.9.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.9.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.9.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.9.mlp.act_fn = SiLUActivation()\n",
      "model.layers.9.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.9.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.10 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.10.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.10.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.10.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.10.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.10.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.10.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.10.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.10.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.10.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.10.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.10.mlp.act_fn = SiLUActivation()\n",
      "model.layers.10.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.10.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.11 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.11.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.11.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.11.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.11.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.11.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.11.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.11.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.11.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.11.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.11.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.11.mlp.act_fn = SiLUActivation()\n",
      "model.layers.11.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.11.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.12 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.12.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.12.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.12.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.12.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.12.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.12.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.12.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.12.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.12.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.12.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.12.mlp.act_fn = SiLUActivation()\n",
      "model.layers.12.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.12.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.13 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.13.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.13.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.13.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.13.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.13.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.13.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.13.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.13.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.13.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.13.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.13.mlp.act_fn = SiLUActivation()\n",
      "model.layers.13.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.13.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.14 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.14.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.14.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.14.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.14.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.14.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.14.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.14.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.14.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.14.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.14.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.14.mlp.act_fn = SiLUActivation()\n",
      "model.layers.14.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.14.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.15 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.15.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.15.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.15.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.15.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.15.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.15.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.15.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.15.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.15.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.15.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.15.mlp.act_fn = SiLUActivation()\n",
      "model.layers.15.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.15.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.16 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.16.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.16.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.16.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.16.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.16.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.16.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.16.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.16.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.16.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.16.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.16.mlp.act_fn = SiLUActivation()\n",
      "model.layers.16.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.16.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.17 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.17.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.17.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.17.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.17.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.17.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.17.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.17.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.17.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.17.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.17.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.17.mlp.act_fn = SiLUActivation()\n",
      "model.layers.17.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.17.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.18 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.18.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.18.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.18.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.18.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.18.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.18.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.18.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.18.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.18.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.18.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.18.mlp.act_fn = SiLUActivation()\n",
      "model.layers.18.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.18.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.19 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.19.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.19.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.19.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.19.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.19.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.19.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.19.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.19.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.19.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.19.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.19.mlp.act_fn = SiLUActivation()\n",
      "model.layers.19.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.19.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.20 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.20.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.20.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.20.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.20.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.20.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.20.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.20.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.20.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.20.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.20.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.20.mlp.act_fn = SiLUActivation()\n",
      "model.layers.20.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.20.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.21 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.21.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.21.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.21.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.21.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.21.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.21.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.21.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.21.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.21.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.21.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.21.mlp.act_fn = SiLUActivation()\n",
      "model.layers.21.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.21.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.22 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.22.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.22.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.22.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.22.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.22.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.22.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.22.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.22.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.22.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.22.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.22.mlp.act_fn = SiLUActivation()\n",
      "model.layers.22.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.22.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.23 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.23.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.23.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.23.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.23.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.23.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.23.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.23.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.23.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.23.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.23.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.23.mlp.act_fn = SiLUActivation()\n",
      "model.layers.23.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.23.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.24 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.24.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.24.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.24.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.24.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.24.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.24.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.24.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.24.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.24.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.24.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.24.mlp.act_fn = SiLUActivation()\n",
      "model.layers.24.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.24.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.25 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.25.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.25.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.25.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.25.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.25.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.25.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.25.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.25.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.25.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.25.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.25.mlp.act_fn = SiLUActivation()\n",
      "model.layers.25.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.25.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.26 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.26.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.26.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.26.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.26.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.26.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.26.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.26.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.26.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.26.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.26.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.26.mlp.act_fn = SiLUActivation()\n",
      "model.layers.26.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.26.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.27 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.27.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.27.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.27.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.27.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.27.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.27.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.27.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.27.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.27.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.27.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.27.mlp.act_fn = SiLUActivation()\n",
      "model.layers.27.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.27.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.28 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.28.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.28.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.28.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.28.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.28.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.28.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.28.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.28.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.28.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.28.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.28.mlp.act_fn = SiLUActivation()\n",
      "model.layers.28.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.28.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.29 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.29.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.29.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.29.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.29.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.29.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.29.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.29.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.29.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.29.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.29.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.29.mlp.act_fn = SiLUActivation()\n",
      "model.layers.29.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.29.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.30 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.30.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.30.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.30.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.30.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.30.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.30.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.30.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.30.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.30.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.30.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.30.mlp.act_fn = SiLUActivation()\n",
      "model.layers.30.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.30.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.31 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.31.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.31.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.31.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.31.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.31.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.31.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.31.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.31.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.31.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.31.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.31.mlp.act_fn = SiLUActivation()\n",
      "model.layers.31.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.31.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.32 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.32.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.32.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.32.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.32.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.32.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.32.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.32.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.32.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.32.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.32.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.32.mlp.act_fn = SiLUActivation()\n",
      "model.layers.32.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.32.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.33 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.33.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.33.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.33.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.33.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.33.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.33.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.33.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.33.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.33.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.33.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.33.mlp.act_fn = SiLUActivation()\n",
      "model.layers.33.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.33.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.34 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.34.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.34.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.34.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.34.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.34.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.34.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.34.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.34.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.34.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.34.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.34.mlp.act_fn = SiLUActivation()\n",
      "model.layers.34.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.34.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.35 = Qwen2_5OmniDecoderLayer(\n",
      "  (self_attn): Qwen2_5OmniAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
      ")\n",
      "model.layers.35.self_attn = Qwen2_5OmniAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
      ")\n",
      "model.layers.35.self_attn.q_proj = Linear(in_features=2048, out_features=2048, bias=True)\n",
      "model.layers.35.self_attn.k_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.35.self_attn.v_proj = Linear(in_features=2048, out_features=256, bias=True)\n",
      "model.layers.35.self_attn.o_proj = Linear(in_features=2048, out_features=2048, bias=False)\n",
      "model.layers.35.self_attn.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "model.layers.35.mlp = Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "model.layers.35.mlp.gate_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.35.mlp.up_proj = Linear(in_features=2048, out_features=11008, bias=False)\n",
      "model.layers.35.mlp.down_proj = Linear(in_features=11008, out_features=2048, bias=False)\n",
      "model.layers.35.mlp.act_fn = SiLUActivation()\n",
      "model.layers.35.input_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.layers.35.post_attention_layernorm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.norm = Qwen2RMSNorm((2048,), eps=1e-06)\n",
      "model.rotary_emb = Qwen2_5OmniRotaryEmbedding()\n",
      "lm_head = Linear(in_features=2048, out_features=151936, bias=False)\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name,\"=\", module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa1b05",
   "metadata": {},
   "source": [
    "```shell\n",
    "           ┌─ gate_proj ── activation ──┐\n",
    "x ──┬──────┤                            ⊙ ── down_proj ── out\n",
    "    │      └─ up_proj   ───────────────┘\n",
    "```\n",
    "\n",
    "+ 不调 gate_proj\n",
    "\n",
    "    trainable params: 31,158,272 || all params: 4,734,622,720 || trainable%: 0.6581\n",
    "\n",
    "+ 调 gate_proj\n",
    "\n",
    "    trainable params: 41,084,928 || all params: 4,744,549,376 || trainable%: 0.8659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d74b3f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 31,158,272 || all params: 4,734,622,720 || trainable%: 0.6581\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"up_proj\", \"down_proj\"],\n",
    "    # target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./r_models\",\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    # per_device_eval_batch_size=batch_size,\n",
    "    bf16=True,\n",
    "    num_train_epochs=3, # 5 -> 2\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=False,\n",
    "    label_names=[\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68724835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    # model=model.thinker,\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    processing_class=processor\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
