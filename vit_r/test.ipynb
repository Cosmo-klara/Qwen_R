{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03071550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "348d379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"food101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6569be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ds[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d768c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'baklava'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f0dde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    use_fast=True, # 修改，有更快的预处理函数\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0205d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        RandomResizedCrop(image_processor.size[\"height\"]),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        Resize(image_processor.size[\"height\"]),\n",
    "        CenterCrop(image_processor.size[\"height\"]),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    example_batch[\"pixel_values\"] = [train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "593b8f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds[\"train\"]\n",
    "val_ds = ds[\"validation\"]\n",
    "\n",
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de523e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "084f54f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b924e30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = ViTForImageClassification(\n",
      "  (vit): ViTModel(\n",
      "    (embeddings): ViTEmbeddings(\n",
      "      (patch_embeddings): ViTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): ViTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x ViTLayer(\n",
      "          (attention): ViTAttention(\n",
      "            (attention): ViTSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (output): ViTSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ViTIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ViTOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=101, bias=True)\n",
      ")\n",
      "vit = ViTModel(\n",
      "  (embeddings): ViTEmbeddings(\n",
      "    (patch_embeddings): ViTPatchEmbeddings(\n",
      "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (encoder): ViTEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x ViTLayer(\n",
      "        (attention): ViTAttention(\n",
      "          (attention): ViTSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output): ViTSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): ViTIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): ViTOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "vit.embeddings = ViTEmbeddings(\n",
      "  (patch_embeddings): ViTPatchEmbeddings(\n",
      "    (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.embeddings.patch_embeddings = ViTPatchEmbeddings(\n",
      "  (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      ")\n",
      "vit.embeddings.patch_embeddings.projection = Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "vit.embeddings.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder = ViTEncoder(\n",
      "  (layer): ModuleList(\n",
      "    (0-11): 12 x ViTLayer(\n",
      "      (attention): ViTAttention(\n",
      "        (attention): ViTSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (output): ViTSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): ViTIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): ViTOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "vit.encoder.layer = ModuleList(\n",
      "  (0-11): 12 x ViTLayer(\n",
      "    (attention): ViTAttention(\n",
      "      (attention): ViTSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (output): ViTSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): ViTIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (intermediate_act_fn): GELUActivation()\n",
      "    )\n",
      "    (output): ViTOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "vit.encoder.layer.0 = ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "vit.encoder.layer.0.attention = ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "vit.encoder.layer.0.attention.attention = ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "vit.encoder.layer.0.attention.attention.query = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.0.attention.attention.key = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.0.attention.attention.value = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.0.attention.output = ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.0.attention.output.dense = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.0.attention.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.0.intermediate = ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "vit.encoder.layer.0.intermediate.dense = Linear(in_features=768, out_features=3072, bias=True)\n",
      "vit.encoder.layer.0.intermediate.intermediate_act_fn = GELUActivation()\n",
      "vit.encoder.layer.0.output = ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.0.output.dense = Linear(in_features=3072, out_features=768, bias=True)\n",
      "vit.encoder.layer.0.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.0.layernorm_before = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.0.layernorm_after = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.1 = ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "vit.encoder.layer.1.attention = ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "vit.encoder.layer.1.attention.attention = ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "vit.encoder.layer.1.attention.attention.query = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.1.attention.attention.key = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.1.attention.attention.value = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.1.attention.output = ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.1.attention.output.dense = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.1.attention.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.1.intermediate = ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "vit.encoder.layer.1.intermediate.dense = Linear(in_features=768, out_features=3072, bias=True)\n",
      "vit.encoder.layer.1.intermediate.intermediate_act_fn = GELUActivation()\n",
      "vit.encoder.layer.1.output = ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.1.output.dense = Linear(in_features=3072, out_features=768, bias=True)\n",
      "vit.encoder.layer.1.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.1.layernorm_before = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.1.layernorm_after = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.2 = ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "vit.encoder.layer.2.attention = ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "vit.encoder.layer.2.attention.attention = ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "vit.encoder.layer.2.attention.attention.query = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.2.attention.attention.key = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.2.attention.attention.value = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.2.attention.output = ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.2.attention.output.dense = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.2.attention.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.2.intermediate = ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "vit.encoder.layer.2.intermediate.dense = Linear(in_features=768, out_features=3072, bias=True)\n",
      "vit.encoder.layer.2.intermediate.intermediate_act_fn = GELUActivation()\n",
      "vit.encoder.layer.2.output = ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.2.output.dense = Linear(in_features=3072, out_features=768, bias=True)\n",
      "vit.encoder.layer.2.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.2.layernorm_before = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.2.layernorm_after = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.3 = ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "vit.encoder.layer.3.attention = ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "vit.encoder.layer.3.attention.attention = ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "vit.encoder.layer.3.attention.attention.query = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.3.attention.attention.key = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.3.attention.attention.value = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.3.attention.output = ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.3.attention.output.dense = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.3.attention.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.3.intermediate = ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "vit.encoder.layer.3.intermediate.dense = Linear(in_features=768, out_features=3072, bias=True)\n",
      "vit.encoder.layer.3.intermediate.intermediate_act_fn = GELUActivation()\n",
      "vit.encoder.layer.3.output = ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.3.output.dense = Linear(in_features=3072, out_features=768, bias=True)\n",
      "vit.encoder.layer.3.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.3.layernorm_before = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.3.layernorm_after = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.4 = ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "vit.encoder.layer.4.attention = ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "vit.encoder.layer.4.attention.attention = ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "vit.encoder.layer.4.attention.attention.query = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.4.attention.attention.key = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.4.attention.attention.value = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.4.attention.output = ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.4.attention.output.dense = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.4.attention.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.4.intermediate = ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "vit.encoder.layer.4.intermediate.dense = Linear(in_features=768, out_features=3072, bias=True)\n",
      "vit.encoder.layer.4.intermediate.intermediate_act_fn = GELUActivation()\n",
      "vit.encoder.layer.4.output = ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.4.output.dense = Linear(in_features=3072, out_features=768, bias=True)\n",
      "vit.encoder.layer.4.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.4.layernorm_before = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.4.layernorm_after = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.5 = ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "vit.encoder.layer.5.attention = ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "vit.encoder.layer.5.attention.attention = ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "vit.encoder.layer.5.attention.attention.query = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.5.attention.attention.key = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.5.attention.attention.value = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.5.attention.output = ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.5.attention.output.dense = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.5.attention.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.5.intermediate = ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "vit.encoder.layer.5.intermediate.dense = Linear(in_features=768, out_features=3072, bias=True)\n",
      "vit.encoder.layer.5.intermediate.intermediate_act_fn = GELUActivation()\n",
      "vit.encoder.layer.5.output = ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.5.output.dense = Linear(in_features=3072, out_features=768, bias=True)\n",
      "vit.encoder.layer.5.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.5.layernorm_before = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.5.layernorm_after = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.6 = ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "vit.encoder.layer.6.attention = ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "vit.encoder.layer.6.attention.attention = ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "vit.encoder.layer.6.attention.attention.query = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.6.attention.attention.key = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.6.attention.attention.value = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.6.attention.output = ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.6.attention.output.dense = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.6.attention.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.6.intermediate = ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "vit.encoder.layer.6.intermediate.dense = Linear(in_features=768, out_features=3072, bias=True)\n",
      "vit.encoder.layer.6.intermediate.intermediate_act_fn = GELUActivation()\n",
      "vit.encoder.layer.6.output = ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.6.output.dense = Linear(in_features=3072, out_features=768, bias=True)\n",
      "vit.encoder.layer.6.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.6.layernorm_before = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.6.layernorm_after = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.7 = ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "vit.encoder.layer.7.attention = ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "vit.encoder.layer.7.attention.attention = ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "vit.encoder.layer.7.attention.attention.query = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.7.attention.attention.key = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.7.attention.attention.value = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.7.attention.output = ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.7.attention.output.dense = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.7.attention.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.7.intermediate = ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "vit.encoder.layer.7.intermediate.dense = Linear(in_features=768, out_features=3072, bias=True)\n",
      "vit.encoder.layer.7.intermediate.intermediate_act_fn = GELUActivation()\n",
      "vit.encoder.layer.7.output = ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.7.output.dense = Linear(in_features=3072, out_features=768, bias=True)\n",
      "vit.encoder.layer.7.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.7.layernorm_before = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.7.layernorm_after = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.8 = ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "vit.encoder.layer.8.attention = ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "vit.encoder.layer.8.attention.attention = ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "vit.encoder.layer.8.attention.attention.query = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.8.attention.attention.key = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.8.attention.attention.value = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.8.attention.output = ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.8.attention.output.dense = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.8.attention.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.8.intermediate = ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "vit.encoder.layer.8.intermediate.dense = Linear(in_features=768, out_features=3072, bias=True)\n",
      "vit.encoder.layer.8.intermediate.intermediate_act_fn = GELUActivation()\n",
      "vit.encoder.layer.8.output = ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.8.output.dense = Linear(in_features=3072, out_features=768, bias=True)\n",
      "vit.encoder.layer.8.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.8.layernorm_before = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.8.layernorm_after = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.9 = ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "vit.encoder.layer.9.attention = ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "vit.encoder.layer.9.attention.attention = ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "vit.encoder.layer.9.attention.attention.query = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.9.attention.attention.key = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.9.attention.attention.value = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.9.attention.output = ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.9.attention.output.dense = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.9.attention.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.9.intermediate = ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "vit.encoder.layer.9.intermediate.dense = Linear(in_features=768, out_features=3072, bias=True)\n",
      "vit.encoder.layer.9.intermediate.intermediate_act_fn = GELUActivation()\n",
      "vit.encoder.layer.9.output = ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.9.output.dense = Linear(in_features=3072, out_features=768, bias=True)\n",
      "vit.encoder.layer.9.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.9.layernorm_before = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.9.layernorm_after = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.10 = ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "vit.encoder.layer.10.attention = ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "vit.encoder.layer.10.attention.attention = ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "vit.encoder.layer.10.attention.attention.query = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.10.attention.attention.key = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.10.attention.attention.value = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.10.attention.output = ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.10.attention.output.dense = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.10.attention.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.10.intermediate = ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "vit.encoder.layer.10.intermediate.dense = Linear(in_features=768, out_features=3072, bias=True)\n",
      "vit.encoder.layer.10.intermediate.intermediate_act_fn = GELUActivation()\n",
      "vit.encoder.layer.10.output = ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.10.output.dense = Linear(in_features=3072, out_features=768, bias=True)\n",
      "vit.encoder.layer.10.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.10.layernorm_before = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.10.layernorm_after = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.11 = ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "vit.encoder.layer.11.attention = ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "vit.encoder.layer.11.attention.attention = ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "vit.encoder.layer.11.attention.attention.query = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.11.attention.attention.key = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.11.attention.attention.value = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.11.attention.output = ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.11.attention.output.dense = Linear(in_features=768, out_features=768, bias=True)\n",
      "vit.encoder.layer.11.attention.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.11.intermediate = ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "vit.encoder.layer.11.intermediate.dense = Linear(in_features=768, out_features=3072, bias=True)\n",
      "vit.encoder.layer.11.intermediate.intermediate_act_fn = GELUActivation()\n",
      "vit.encoder.layer.11.output = ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "vit.encoder.layer.11.output.dense = Linear(in_features=3072, out_features=768, bias=True)\n",
      "vit.encoder.layer.11.output.dropout = Dropout(p=0.0, inplace=False)\n",
      "vit.encoder.layer.11.layernorm_before = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.encoder.layer.11.layernorm_after = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "vit.layernorm = LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "classifier = Linear(in_features=768, out_features=101, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name,\"=\", module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73cacbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 667,493 || all params: 86,543,818 || trainable%: 0.7713\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"classifier\"],\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a934b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./r_models\",\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=True,\n",
    "    num_train_epochs=2, # 5 -> 2\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    label_names=[\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3068c0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/peft/peft_model.py\", line 921, in forward\n    return self.get_base_model()(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/utils/generic.py\", line 918, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py\", line 668, in forward\n    outputs: BaseModelOutputWithPooling = self.vit(\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/utils/generic.py\", line 1072, in wrapper\n    outputs = func(self, *args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py\", line 485, in forward\n    encoder_outputs: BaseModelOutput = self.encoder(embedding_output, head_mask=head_mask)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py\", line 368, in forward\n    hidden_states = layer_module(hidden_states, layer_head_mask)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/modeling_layers.py\", line 94, in __call__\n    return super().__call__(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py\", line 350, in forward\n    layer_output = self.intermediate(layer_output)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py\", line 310, in forward\n    hidden_states = self.dense(hidden_states)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 296.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 68.75 MiB is free. Process 69705 has 1.13 GiB memory in use. Process 77552 has 1.13 GiB memory in use. Process 110807 has 7.74 GiB memory in use. Process 110814 has 956.00 MiB memory in use. Including non-PyTorch memory, this process has 12.58 GiB memory in use. Of the allocated memory 12.09 GiB is allocated by PyTorch, and 59.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexport CUDA_VISIBLE_DEVICES=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2,3\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      3\u001b[0m     model,\n\u001b[1;32m      4\u001b[0m     args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mcollate_fn,\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2672\u001b[0m )\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2680\u001b[0m ):\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/trainer.py:4020\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   4019\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 4020\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4022\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   4023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4024\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4025\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   4026\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/trainer.py:4110\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4108\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   4109\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m-> 4110\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   4112\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   4113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:193\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    192\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 193\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:212\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparallel_apply\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any\n\u001b[1;32m    211\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:126\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    124\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 126\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/_utils.py:733\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/peft/peft_model.py\", line 921, in forward\n    return self.get_base_model()(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/utils/generic.py\", line 918, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py\", line 668, in forward\n    outputs: BaseModelOutputWithPooling = self.vit(\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/utils/generic.py\", line 1072, in wrapper\n    outputs = func(self, *args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py\", line 485, in forward\n    encoder_outputs: BaseModelOutput = self.encoder(embedding_output, head_mask=head_mask)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py\", line 368, in forward\n    hidden_states = layer_module(hidden_states, layer_head_mask)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/modeling_layers.py\", line 94, in __call__\n    return super().__call__(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py\", line 350, in forward\n    layer_output = self.intermediate(layer_output)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py\", line 310, in forward\n    hidden_states = self.dense(hidden_states)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 296.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 68.75 MiB is free. Process 69705 has 1.13 GiB memory in use. Process 77552 has 1.13 GiB memory in use. Process 110807 has 7.74 GiB memory in use. Process 110814 has 956.00 MiB memory in use. Including non-PyTorch memory, this process has 12.58 GiB memory in use. Of the allocated memory 12.09 GiB is allocated by PyTorch, and 59.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=image_processor,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaf022e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
